# 쿠버 기초

 90 ~ 153

쿠버네티스 1.18버전 업데이트로 인해 kubectl 몀령이 변경됨

kubectl run -> kubectl create deploy 로 변경 --port 빼고

--dry-run 명령은 --dry-run=client



https://arisu1000.tistory.com/27834





1. 마스터 노드

   - 설정하기

   ```
   vi install.sh
   
   sudo apt-get update && sudo apt-get install -y apt-transport-https curl
   curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
   cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
   deb https://apt.kubernetes.io/ kubernetes-xenial main
   EOF
   sudo apt-get update
   sudo apt-get install -y kubelet kubeadm kubectl
   sudo apt-mark hold kubelet kubeadm kubectl
   
   저장 후 
   
   bash install.sh
   
   설치 후
   
   sudo kubeadm init
   sudo swapoff -a
   sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab 
   
   노드 초기화 후 나오는 메세지 중 아래 실행
   
   mkdir -p $HOME/.kube
   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
   sudo chown $(id -u):$(id -g) $HOME/.kube/config
   
   중요! Pod Network 추가
   kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
   
   
   kubeadm join 10.0.0.178:6443 --token 1insnt.r880f4ingbwjjcmg \
       --discovery-token-ca-cert-hash sha256:8999b49f36a2c9a30f0ecf95c4bbb78bd2562b4f189924b62def3995b829912d
   ```



마스터노드 install.sh

```
sudo apt-get update && sudo apt-get install -y apt-transport-https curl
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
sudo kubeadm init
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"

```



워커노드 install.sh

```
sudo apt-get update && sudo apt-get install -y apt-transport-https curl
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

```







1. 슬레이브 노드 

   - 설정하기

   ```
   1. 도커를 설치한다
   sudo apt-get update
   sudo apt install docker.io
   
   2. 마스터 노드에서 쿠버네티스를 설치한 것과 동일하게 설치한다.
   3. init 명령어 전까지만 수행한다.
   
   마스터에서 init 후 나온 다음과 같은 메시지로 워커 노드를 참가 시킨다.
   
   sudo kubeadm join 10.0.2.15:6443 --token xwvbff.5xc67j8qc6ohl2it \
   --discovery-token-ca-cert-hash
   sha256:e19e9263aeb2340a602c2057966b71551e01a5e287d3f23b05073c7b248932e1
   ```

   

2. 쿠버네티스 명령어

   - 연결된 노드들의 상태를 확인

   ```
   kubectl get nodes
   ```

   - 간단한 애플리케이션 실행 및 확인

   ```
   kubectl create deploy --image=nginx
   kubectl get pod
   kubectl port-forward nginx-XXXXXXXXXX-XXXXX 8080:80
   ```

   - 포드 확인

   ```
   kubectl get pods
   ```

   - 서비스 확인

   ```
   kubectl get services
   ```

   - 애플리케이션 수평 스케일링

   ```
    kubectl scale deploy http-go --replicas=3
   ```

   - 디플로이 확인

   ```
    kubectl get deploy
   ```

   - 직접 앱에 접근하기

   ```
   kubectl get pods 로 아이피 확인
   kubectl exec http-go-XXXXXX-bt4xq -- curl -s http://10.109.140.155:8080
   ```

   - 앱의 위치 확인

   ```
   kubectl get pod -o wide
   ```

   - 포드의 자세한 내용 살펴보기

   ```
   kubectl describe pod http-go-XXXXXX-bt4xq
   ```

   - 모든 서비스 지우기

   ```
   kubectl delete all --all
   ```

   - 쿠버니테스 주요 컴포넌트 확인하기

   ```
    kubectl get pod -n kube-system
   ```

   - 라벨 함께 출력

   ```
   kubectl get pods --show-labels
   ```

   - 디플로이먼트에서 생성된 포드의 이미지를 변경

   ```
   kubectl set image deployment my-nginx-deployment nginx=nginx:1.11 --record
   
   --record 옵션으로 이전 리비전 정보 생성
   
   nginx라는 이름을 가지는 컨테이너의 이미지를 nginx:1.11으로 변경
   
   이미지를 업데이트하면 레플리카셋이 새롭게 생성됨
   
   이전 버전의 레플리카셋을 삭제하지 않고 남겨둠
   
   디플로이먼트는 포드의 정보가 변경되어 업데이트가 발생하면, 이전의 정보를 리비전으로서 보존한다.
   ```

   - 리비전 정보 조회

   ```
   kubectl rollout history deployment my-nginx-deployment
   ```

   - 이전 버전의 레플리카셋으로 롤백

   ```
   kubectl rollout undo deployment my-nginx-deployment --to-revision=1
   
   되돌리려는 리비전 번호 입력
   ```

   - 





1. 쿠버네티스에 앱 실행해보기

   - go 언어로 main.go 작성

   ```
   package main
   
   import (
           "fmt"
           "github.com/julienschmidt/httprouter"
           "net/http"
           "log"
           "os"
   )
   
   func Index(w http.ResponseWriter, r *http.Request, _ httprouter.Params){
           hostname,err:=os.Hostname()
           if err == nil {
                   fmt.Fprint(w,"Welcome! " + hostname +"\n")
           } else{
                   fmt.Fprint(w, "Welcome! Error\n")
           }
   }
   
   func main() {
           router := httprouter.New()
           router.GET("/",Index)
   
           log.Fatal(http.ListenAndServe(":8080",router))
   }
   ```

   - Go 언어 설치 및 빌드

   ```
   apt install golang
   go get github.com/julienschmidt/httprouter
   go build main.go
   
   ./main
   
   외부에서 접속해서 확인
   
   ```

   - dockerfile 작성

   ```
   FROM golang:1.11
   WORKDIR /usr/src/app
   COPY main /usr/src/app
   CMD ["/usr/src/app/main"]
   ```

   - 컨테이너 이미지 만들기

   ```
   docker build -t http-go
   ```

   - 컨테이너 실행해서 확인

   ```
   docker run -d -p 8080:8080 --rm http-go
   ```

   - 도커허브에 컨테이너 푸시하기

   ```
   docker tag http-go dalgudcks/http-go
   docker login
   
   docker push dalgudcks/http-go
   ```

   - 명령어에 몇 가지 옵션으로 디스크립션을 간단히 전달하여 한줄로 앱을 실행

   ```
    kubectl create deploy http-go --image=dalgudcks/http-go
   ```

   - 로드밸런서라는 서비스를 작성하여 외부 로드 밸런서를 생성한다. 

   ```
    kubectl expose deployment http-go --type=LoadBalancer --name http-go-svc --port=8080 --target-port=8080
   
   kubectl get services
   ```

- 앱에 접근하기
  
```
   kubectl exec http-go-XXXXXX-bt4xq -- curl -s http://10.109.140.155:8080
   
   curl 명령어로 요청
   external IP를 할당 받지 못했기 때문에 포드의 힘을 빌려 요청한다.
```



kube-system의 역할

### etcd



### Pod

- yaml로 포드 디스크립터 만들기

  포드 정의

  1. apiVersion: 쿠버네티스 api의 버전을 가리킴
  2. kind: 어떤 리소스 유형인지 결정(포드 레플리카컨트롤러, 서비스 등)
  3. 메타데이터: 포드와 관련된 이름, 네임스페이스, 레이블, 그 밖의 정보 존재
  4. 스펙: 컨테이너, 볼륨 등의 정보
  5. 상태: 포드의 상태, 각 컨테이너의 설명 및 상태, 포드 내부의 IP 및 그 밖의 기본 정보 등



- 포드에서 YAML 파일 불러오기

  - ```
    kubectl get pod http-go -o yaml
    ```

- 디스크립터 작성하기

  - ```
    http-go-pod.yaml
    # 이 디스크립터는 쿠버네티스 API v1를 사용
    apiVersion: v1
    # 리소스 포드에 대한 설명
    kind: Pod
    metadata:
    # 포드의 이름
     name: http-go
    spec:
     containers:
    # 생성할 컨테이너의 컨테이너 이미지
     - image: gasbugs/http-go
       name: http-go
       ports:
    # 응답 대기할 애플리케이션 포트
       - containerPort: 8080
         protocol: TCP
    ```
  
- kubectl에 디스크립터 작성 요령 확인 가능
  
  ```
    kubectl explain pods
  ```
  
- 디스크립터를 사용해 포드 생성
  
  ```
    kubectl create -f http-go-pod.yaml
  ```
  
- kubectl log로 포드의 로그 가져오기
  
  ```
    kubectl logs http-go
  ```
  
- 컨테이너에서 호스트로 포트 포워딩
  
  - 디버깅 혹은 다른 이유로 서비스를 거치지 않고 특정 포드와 통신하고 싶을 때 사용
    -  kubectl port-forward 명령으로 수행
  
  ```
    kubectl port-forward http-go 8080:8080
    
    kubectl port-forward http-go 8888:8080 &
    
    #컨테이너 8888 포트를 pod의 8080 포트로 전달
    
    bg
    curl 127.0.0.1:8888
  ```
  
- 포드에 주석 추가하기
  
  - 각 포드나 API 객체 설명이 추가
    - 클러스터를 사용하는 모든 사람이 각 객체의 정보를 빠르게 확인 가능
    - 예를 들어 객체를 만든 사람의 이름을 지정
    - 공동 작업 가능
    - 총 256KB까지 포함 가능
  
  ```
     kubectl annotate pod http-go key="test1234"
    
     kubectl get pod http-go -o yaml #확인
  ```
  
- 포드 삭제
  
  ```
    kubectl delete pod <포드 이름>
    
    kubectl get pod 로 조회
    
    kubectl delete pod --all
  ```
  
  
  

  
- Liveness, Readiness and Startup Probes

  - Liveness Probe

    - 컨테이너가 살았는지 판단하고 다시 시작하는 기능
    - 컨테이너의 상태를 스스로 판단하여 교착 상태에 빠진 컨테이너를 재시작 
    - 버그가 생겨도 높은 가용성을 보임

    - Liveness 커맨드 설정 - 파일 존재 여부 확인

      실행 성공 시 0 (컨테이너 유지)

      실패하면 그 외 값 출력 (컨테이너 재시작)

      ```
      exec-liveness.yaml
      
      apiVersion: v1
      kind: Pod
      metadata:
              labels:
                test: liveness
              name: liveness-exec
      spec:
              containers:
              - name: liveness
                image: k8s.gcr.io/busybox
                args:
                - /bin/sh
                - -c
                - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
                livenessProbe:
                        exec:
                                command:
                                - cat
                                - /tmp/healthy
                        initialDelaySeconds: 5
                        periodSeconds: 5
      
      ```

      - Liveness 웹 설정 - http 요청 확인

        서버 응답 코드가 200이상 400미만 (컨테이너 유지)

        서버 응답 코드가 그 외 (컨테이너 재시작)

        ```
        http-liveness.yaml
        
        apiVersion: v1
        kind: Pod
        metadata:
                labels:
                  test: liveness
                name: liveness-http
        spec:
                containers:
                - name: liveness
                  image: k8s.gcr.io/liveness
                  args:
                  - /server
                  livenessProbe:
                          httpGet:
                                  path: /healthz
                                  port: 8080
                                  httpHeaders:
                                  - name: Custom-Header
                                    value: Awesome
                                  
                          initialDelaySeconds: 3
                          periodSeconds: 3
        
        ```

        

  

  - Readiness Probe

    - 포드가 준비된 상태에 있는지 확인하고 정상 서비스를 시작하는 기능
    - 포드가 적절하게 준비되지 않은 경우 로드밸런싱을 하지 않음

  - Startup Probe

    - 애플리케이션의 시작 시기 확인하여 가용성을 높이는 기능
    - Liveness와 Readiness의 기능을 비활성화

    

## 레이블과 셀렉터

- 레이블
  - 모든 리소스를 구성하는 매우 간단하면서도 강력한 쿠버네티스 기능
  - 리소스에 첨부하는 임의의 키/값 쌍(예 app: test)
  - 레이블 셀렉터를 사용하면 각종 리소스를 필터링하여 선택할 수 있음
  -  리소스는 한 개 이상의 레이블을 가질 수 있음
  - 리소스를 만드는 시점에 레이블을 첨부
  - 기존 리소스에도 레이블의 값을 수정 및 추가 가능



포드 생성 시 레이블을 지정하는 방법

```
http-go-pod-v2.yaml

apiVersion: v1
kind: Pod
metadata:
	name: http-go-v2
	labels:
		creation_method: manual
		env: prod
spec:
	containers:
	- image: dalgudcks/http-go
	  name: http-go
	  ports:
	  - containerPort: 8080
	    protocol: TCP
```



- 레이블 추가 및 수정하는 방법

  - 새로운 레이블 추가

  ```
   kubectl label pod http-go-v2 test=foo
  ```

  - 기존 레이블 수정

  ```
  kubectl label pod http-go-v2 rel=beta --overwrite
  ```

  - 레이블 삭제

  ```
   kubectl label pod http-go-v2 rel-
  ```

  - 레이블 확인하기

  ```
   kubectl get pod --show-labels
  ```

  - 특정 레이블 컬럼으로 확인

  ```
   kubectl get pod -L app,rel
  ```

  - 레이블 필터링 검색

  ```
  kubectl get pod --show-labels -l 'env'
  kubectl get pod --show-labels -l '!env'
  kubectl get pod --show-labels -l 'env!=test'
  kubectl get pod --show-labels -l 'env!=test,rel=beta'
  
  ```

  

##  ReplicaSet



**레플리케이션컨트롤러**

---



- 레플리케이션컨트롤러

  - 포드가 항상 실행되도록 유지하는 쿠버네티스 리소스
  - 노드가 클러스터에서 사라지는 경우 해당 포드를 감지하고 대체 포드 생성
  - 실행 중인 포드의 목록을 지속적으로 모니터링으로 하고 '유형'의 실제 포드 수가 원하는 수와 항상 일치하는지 확인

  

- 레플리케이션컨트롤러의 세 가지 요소
  - 레플리케이션컨트롤러가 관리하는 포드 범위를 결정하는 레이블 셀렉터
  - 실행해야 하는 포드의 수를 결정하는 복제본 수
  - 새로운 포드의 모양을 설명하는 포드 템플릿



- 레플리케이션컨트롤러의 장점
  - 포드가 없는 경우 새 포드를 항상 실행
  - 노드에 장애 발생 시 다른 노드에 복제본 생성
  - 수동,자동으로 수평 스케일링



- Yaml 작성

```
apiVersion: v1
kind: ReplicationController
metadata:
        name: rc-nodejs
spec:
		# 복제본 수 : 실행해야 하는 포드의 수를 결정
        replicas: 3
        selector:
          #라벨 셀렉터 : 레플리케이션컨트롤러가 관리하는 포드 범위를 결정
          app: nodejs
        template:
          #포드 템플릿 : 새로운 포드의 모양을 설명
          metadata:
                labels:
                        app: nodejs
          spec:
                containers:
                - name: nodejs
                  image: dalgudcks/nodejs
                  ports:
                  - containerPort: 8080

```



-  kubectl delete pod rc-http-go-znmlz 로 파드를 지우면 다시 생성시킴.
- 레플리케이션 정보 확인

```
 kubectl describe rc rc-http-go
```



- 레플리카컨트롤러의 관리 레이블 벗어나기

```
 kubectl get pod -L app
 
  kubectl label pod rc-http-go-zhdj5 app=http-go2 --overwrite
```

​	포드의 레이블이 변경되어 관리 밖으로 벗어나면 이를 건드리지 않고 새로운 포드를 생성



- 레플리케이션컨트롤러 설정 바꾸기

  - 설정파일 접근

  ```
  kubectl edit rc rc-http-go
  ```

  - vim이 열리면 replicas 개수를 3에서 20개로 수정하고 저장 종료
  - 포드의 수 변화 확인

  - 명령어로 레플레케이션컨트롤러 설정 변경

  ```
  kubectl scale rc rc-http-go --replicas=10
  ```

- 레플리케이션컨트롤러 삭제 

  ```
  kubectl delete rc rc-http-go
  ```

- 실행 시키고 있는 포드는 유지하면서 지우기

```
 kubectl delete rc rc-http-go --cascade=false
```



- 실습하기

```
apiVersion: v1
kind: ReplicationController
metadata:
  name: http-go
spec:
  replicas: 3
  selector:
    app: http-go
  template:
    metadata:
      name: http-go
      labels:
        app: http-go
    spec:
      containers:
      - name: http-go
        image: gasbugs/http-go
        ports:
        - containerPort: 8080
```



**레플리카셋**

---



- 레플리카셋은 차세대 레플리케이션컨트롤러로 레플리케이션컨트롤러를 완전히 대체 가능함

- 레플리카셋과 레플리케이션컨트롤러는 거의 동일하게 동작함
- 레플리카셋이 더 풍부한 표현식 포드 셀렉터 사용 가능



- 레플리카셋 생성
  - 대부분의 요소는 거의 비슷함
  - apiVersion: apps/v1beta2
  - kind: ReplicaSet
  - matchExpressions: 레이블을 매칭하는 별도의 표현 방식 존재



- 레플리카셋 조회

```
kubectl get rs
```



- 레플리카셋 상세 조회

```
$kubectl describe rs http-go-rs
```



- 레플리카셋 삭제

```
kubectl delete rs http-go-rs
```



- 실습하기

```
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: rs-nginx
spec:
  # modify replicas according to your case
  replicas: 3
  selector:
    matchLabels:
      app: rs-nginx
  template:
    metadata:
      labels:
        app: rs-nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports: 
        - containerPort: 80
```





## Deployment

- 애플리케이션을 다운 타입 없이 업데이트 가능하도록 도와주는 리소스
- 레플리카셋과 레플리케이션컨트롤러 상위에 배포되는 리소스



- 모든 포드를 업데이트하는 방법
  - 잠깐의 다운타임이 발생한다. (새로운 포드를 실행시키고 작업이 완료되면 오래된 포드 삭제)
  - 롤링 업데이트



- 작성 요령
  - 포드의 metadata 부분과 spec 부분을 그대로 옮김
  - Deployment의 spec.template에는 배포할 포드를 설정
  - replicas에는 이 포드를 몇 개를 배포할지 명시
  - label은 디플로이먼트가 배포한 포드를 관리하는데 사용됨

```
apiVersion: apps/v1
kind: Deployment
metadata:
        name: nginx-deployment
        labels:
                app: nginx
spec:
        replicas: 3
        selector:
                matchLabels:
                        app: nginx
        template:
                metadata:
                        labels:
                         app: nginx
                spec:
                        containers:
                        - name: nginx
                          image: nginx:1.7.9
                          ports:
                          - containerPort: 80

```



- 디플로이먼트 스케일링

  - ```
    kubectl edit deploy <deploy name> #yaml 파일 직접 수정해서 replicas 수 조정
    ```

  - ```
    kubectl scale deploy <deploy name> --replicas=<number>
    #replicas 수 조정
    ```



- 실습하기

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-jenkins
  labels:
    app: jenkins-test
spec:
  replicas: 3
  selector:
    matchLabels:
      app: jenkins-test
  template:
    metadata:
      labels:
        app: jenkins-test
    spec:
      containers:
      - name: jenkins
        image: jenkins
        ports:
        - containerPort: 8080
```







## 애플리케이션 롤링 업데이트와 롤백

- 기존 모든 포드를 삭제 후 새로운 포드 생성
  - 잠깐의 다운 타임 발생
- 새로운 포드를 실행시키고 작업이 완료되면 오래된 포드를 삭제
  - 새 버전을 실행하는 동안 구 버전 포드와 연결
  - 서비스의 레이블셀렉터를 수정하여 간단하게 수정가능



- 레플리케이션컨틀롤러가 제공하는 롤링 업데이트

  - 이전에는 kubectl을 사용해 스케일링을 사용하여 수동으로 롤링 업데이트 진행 가능

  - kubectl이 중단되면 업데이트는 어떻게 되나?

  - 레플리케이션컨트롤러 또는 레플리카셋을 통제할 수 있는 시스템이 필요

    

- 실습

  - 디플로이먼트 생성  YAML 작성

  ```
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: http-go
    labels:
      app: http-go
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: http-go
    template:
      metadata:
        labels:
          app: http-go
      spec:
        containers:
        - name: http-go
          image: gasbugs/http-go:v1
          ports:
          - containerPort: 8080
  ```

  ```
  kubectl create -f http-go-deployment.yaml --record=true
  kubectl get deployment
  kubectl get rs
  kubectl get pod
  
  kubectl rollout status deployment http-go #rollot을 통해서 상태 확인 가능
  ```

  

- 디플로이먼트 업데이트 전략

  - Rolling Update (기본)
    - 오래된 포드를 하나씩 제거하는 동시에 새로운 포드 추가
    - 요청을 처리할 수 있는 양은 그대로 유지
    - 반드시 이전 버전과 새 버전을 동시에 처리 가능하도록 설계한 경우에만 사용
  - Recreate
    - 새 포드를 만들기 전에 이전 포드를 모두 삭제
    - 여러 버전을 동시에 실행 불가능
    - 잠깐의 다운 타임 존재
  - 업데이트 과정을 보기 위해 업데이트 속도 조절

  ```
  kubectl patch deployment http-go -p '{"spec": {"minReadySeconds": 10}}'
  ```

  - 디플로이먼트를 모니터하는 프로그램 실행
  
  ```
  while true; curl <ip>; sleep 1; done
  
  ```
  
  
  
  - 이미지 업데이트 실행
  
  ```
  kubectl set image deployment http-go-deployment http-go=dalgudcks/http-go:v2
  
  ```
  
  - 업데이트한 이력을 확인
  
  ```
  kubectl rollout history deployment http-go
  ```
  
  
  
  - 롤백 실행하기
    - 롤백을 실행하면 이전 업데이트 상태로 돌아감
    - 롤백을 하여도 히스토리의 리비전 상태는 이전 상태로 돌아가지 않음
  
  ```
  $ kubectl set image deployment http-go http-go=gasbugs/http-go:v3
  deployment.extensions/http-go image updated
  
  $ kubectl rollout undo deployment http-go
  deployment.extensions/http-go
  
  $ kubectl exec http-go-7dbcf5877-d6n6p curl 127.0.0.1:8080
  Welcome! http-go:v2
  
  $ kubectl rollout undo deployment http-go --to-revision=1
  deployment.extensions/http-go
  ```
  
  

- 롤링 업데이터 전략 세부 설정
  - maxSurge
    - 기본값 25%,개수로도 설정 가능
    - 최대로 추가 배포를 허용할 개수 설정
    - 4개인 경우 25%이면 1개가 설정 ( 총 개수 5개까지 동시 포드 운영 )
  - maxUnavailable
    - 기본값 25%, 개수로도 설정 가능
    - 동작하지 않는 포드의 개수 설정
    - 4개인 경우 25%이면 1개가 설정(총 개수 4-1개는 운영해야함)



- 롤아웃 일시중지와 재시작

  - 업데이트 중에 일시정지 하길 원하는 경우

  ```
  kubectl rollout pause deployment http-go
  ```

  - 업데이트 일시중지 중 취소

  ```
   kubectl rollout undo deployment http-go
  ```

  - 업데이트 재시작

  ```
  kubectl rollout resume deployment http-go
  ```



- 업데이트를 실패한 경우

  - 업데이트를 실패하는 케이스
    - 부족한 할당량
    - 레디네스 프로브 실패
    - 이미지 가져오기 오류
    - 권한 부족
    - 제한 범위
    - 응용 프로그램 런타임 구성 오류
  - 업데이트를 실패하는 경우에는 기본적으로 600초 후에 업데이를 중지한다.

  ```
  spec:
    processDeadlineSeconds: 600
  ```

  

- 실습해보기

```
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    run: alpine-deploy
  name: alpine-deploy
spec:
  replicas: 10
  selector:
    matchLabels:
      run: alpine-deploy
  strategy: 
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 50%
      maxUnavailable: 50%
  template:
    metadata:
      creationTimestamp: null
      labels:
        run: alpine-deploy
    spec:
      containers:
      - image: alpine:3.4
        name: alpine-deploy
        resources: {}
status: {}
```





## Namespaces

- 리소스를 각각의 분리된 영역으로 나누기 좋은 방법
- 여러 네임스페이스를 사용하면 복잡한 쿠버네티스 시스템을 더 작은 그룹으로 분할
- 멀티 테넌트 환경을 분리하여 리소스를 생산,개발,QA 환경 등으로 사용
- 리소스 이름은 네임스페이스 내에서만 고유 명칭 사용



현재 클러스터의 기본 네임스페이스 확인

```
kubectl get ns
```



각 네임스페이스 상세 내용 확인

- kubectl get을 옵션없이 사용하면 default 네임스페이스에 질의
- 다른 사용자와 분리된 환경으로 타인의 접근을 제한
- 네임스페이스 별로 리소스 접근 허용과 리소스 양도 제어 가능
- --namespace나 -n을 사용하여 네임스페이스 별로 확인이 가능

```
kubectl get po --namespace kube-system
```



yaml 파일로 네임스페이스 만들기

- test_ns.yaml 파일을 생성하고 create 를 사용하여 생성

```
apiVersion: v1
kind: Namespace
metadata:
	#네임스페이스 이름
	name:teset-ns
```



```
kubectl create -f test_ns.yaml
kubectl get ns
```



kubectl 명령어로 yaml 없이 바로 네임스페이스 생성 가능

```
kubectl create namespace "test-namespace"
```



전체 네임스페이스를 대상으로 kubectl을 실행하는 방법

```
kubectl get pod --all-namespaces
```



- 실습하기
  - ns-jenkins 네임스페이스를 생성하고 jenkins 포드를 배치하기

```
jenkins-ns.yaml

apiVersion: v1
kind: Namespace
metadata:
 name: ns-jenkins
---
apiVersion: v1
kind: Pod
metadata:
 name: jenkins
 namespace: ns-jenkins
spec:
 containers:
 - name: jenkins
   image: jenkins
   ports:
   - containerPort: 8080

```



## Services

- 포드의 문제점
  - 포드는 일시적으로 생성한 컨테이너의 집합
  - 때문에 포드가 지속적으로 생겨났을 때 서비스를 하기에 적합하지 않음
  - IP주소의 지속적인 변동, 로드밸런싱을 관리해 줄 또 다른 개체가 필요



- 요구사항
  - 외부 클라이언트가 몇 개이든지 프론트엔드 포드로 연결
  - 프론트엔드는 다시 백엔드 데이터베이스로 연결
  - 포드의 IP가 변경될 때마다 재설정 하지 않도록 해야함



- 생성방법
  - kubectl의 expose가 가장 쉬움
  - YAML을 통해 버전관리 가능

```
http-go-svc.yaml

apiVersion: v1
kind: Service
metadata:
 name: http-go-svc
spec:
 ports:
 - port: 80
   targetPort: 8080
 selector:
  app: http-go
```

```
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: http-go-rs
  labels:
    app: http-go
    tier: frontend
spec:
  # 케이스에 따라 레플리카를 수정한다.
  replicas: 3
  selector:
    matchLabels:
      tier: http-go
  template:
    metadata:
      labels:
        tier: http-go
    spec:
      containers:
      - name: http-go
        image: dalgudcks/http-go

```



```
kubectl create -f http-go-svc.yaml

kubectl create -f http-go-rs.yaml

kubectl get svc
```



- 서비스의 기능 확인

```
kubectl exec <포드 이름> --curl

kubectl exec http-go-rs-4152m -- curl 10.12.0.237:80 -s

(kubectl get pods -o wide로 확인)
kubectl exec http-go-rs-zfqmf -- curl 10.42.0.1:8080 -s

```



- 포드 간의 통신을 위한 ClusterIP
  - 다수의 포드를 하나의 서비스로 묶어서 관리

  ```yaml
  예시) 프론트엔드 -(서비스)- 백엔드 -(서비스)- 데이터베이스
  
  apiVersion: v1
  kind: Service
  metadata:
   name: back-end
   
  spec:
   type: ClusterIP
   ports:
   - targetPort:80
     port: 80
     
   selector:
    app: myapp
    type: back-end
  
  --------------------
  apiVersion: v1
  kind: Service
  metadata:
   name: db
   
  spec:
   type: ClusterIP
   ports:
   - targetPort: 3006
     port: 3306
     
   selector:
    app: mysql
    type: db
  ```

  

- 서비스의 세션 고정하기
  - 서비스가 다수의 포드로 구성하면 웹서비스의 세션이 유지되지 않음
  - 이를 위해 처음 들어왔던 클라이언트 IP를 그대로 유지해주는 방법이 필요
  - **sessionAffinity: ClientIP**라는 옵션을 주면 해결 

```
apiVersion: v1
kind: Service
metadata:
 name: http-go-svc
spec:
 sessionAffinity: ClientIP
 ports:
 - port: 80
   targetPort: 8080
 selector:
  app: http-go
```

```
$ kubectl exec http-go-rs-4l52m -- curl 10.12.0.237:80
% Total % Received % Xferd Average Speed Time Time Time Current
Dload Upload Total Spent Left Speed
100 27 0 27 0 0 1492 0 --:--:-- --:--:-- --:--:-- 1588
Welcome! http-go-rs-vsf5n

$ kubectl exec http-go-rs-4l52m -- curl 10.12.0.237:80
% Total % Received % Xferd Average Speed Time Time Time Current
Dload Upload Total Spent Left Speed
100 27 0 27 0 0 1492 0 --:--:-- --:--:-- --:--:-- 1588
Welcome! http-go-rs-vsf5n

$ kubectl exec http-go-rs-4l52m -- curl 10.12.0.237:80
% Total % Received % Xferd Average Speed Time Time Time Current
Dload Upload Total Spent Left Speed
100 27 0 27 0 0 1492 0 --:--:-- --:--:-- --:--:-- 1588
Welcome! http-go-rs-vsf5n

모두 같은 Pod로만 요청
```



- svc 자세히 확인 (포트 및 엔드포인트)

```
 kubectl describe svc http-go-svc
```

- 외부 IP 연결 설정 YAML
  - Sesrvice와 Endpoints 리소스 모두 생성 필요

```
external-svc.yaml

apiVersion: v1
kind: Service
metadata:
        name: external-service
spec:
        ports:
        - port: 80
```

```
external-service.yaml

apiVersion: v1
kind: Endpoints
metadata:
        name: external-service
subsets:
        - addresses:
          - ip: 11.11.11.11
          - ip: 22.22.22.22
          ports:
          - port: 80
```



-  서비스 노출하는 세 가지 방법
  - NodePort: 노드의 자체 포트를 사용하여 포드로 리다이렉션
  - LoadBalancer: 외부 게이트웨이를 사용해 노드 포트로 리다이렉션
  - Ingress: 하나의 IP주소를 통해 여러 서비스를 제공하는 특별한 메커니즘
<<<<<<< HEAD



- 노드포트 생성하기
  - 서비스 yaml 파일을 작성
  - type에 NodePort를 지정
  - 30000-32767포트만 사용가능

```
http-go-np.yaml

apiVersion: v1
kind: Service
metadata:
 name: http-go-svc
spec:
 type: NodePort
 ports:
 - port: 80
   targetPort: 8080
   nodePort: 30001
 selector:
  app: http-go
```

```
kubectl get svc
```



- 참고 사이트
- https://crystalcube.co.kr/199

```
kubectl cluster info
```





# 외부 노드 JS로 우회해서 클러스터 접근하기

- API Doc

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#daemonset-v1-apps

- 테스트 JSON

```
{
    "apiVersion":"apps/v1",
    "kind":"Deployment",
    "metadata":{
        "name":"nginx-deployment",
        "labels":{
            "app":"nginx"
        }
    },
    "spec": {
    "replicas" : 3,
    "selector": {
        "matchLabels" : {
            "app":"nginx"
        }
    },
    "template" : {
    "metadata" : {
        "labels" : {
            "app":"nginx"
        }
    },
    "spec":{
        "containers":[
            {
                "name":"ngnix",
                "image":"nginx:1.7.9",
                "ports":[
                  {
                    "containerPort": 80 
                }
                ]
            }
        ]
    }
 }
}
}
```



 내부에서 curl 로 되는지 테스트하기

```
curl --request POST > --url 'http://localhost:8001/apis/apps/v1/namespaces/default/deployments' \
--header 'Content-Type: application/json' \
--data-raw '{
    "apiVersion":"apps/v1",
    "kind":"Deployment",
    "metadata":{
        "name":"nginx-deployment",
        "labels":{
            "app":"nginx"
        }
    },
    "spec": {
    "replicas" : 3,
    "selector": {
        "matchLabels" : {
            "app":"nginx"
        }
    },
    "template" : {
    "metadata" : {
        "labels" : {
            "app":"nginx"
        }
    },
    "spec":{
        "containers":[
            {
                "name":"ngnix",
                "image":"nginx:1.7.9",
                "ports":[
                  {
                    "containerPort": 80 
                }
                ]
            }
        ]
    }
 }
}
}'
```





- Nodejs Express 경로



148  mkdir nodejs
  149  ls
  150  cd nodejs
  151  node -v
  152  curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.11/install.sh | bash
  153  nvm
  154  source ./bashrc
  155  sources ./bashrc
  156  cd
  157  sources ./bashrc
  158  source ./bashrc
  159  source ~/bashrc
  160  source ~/.bashrc
  161  nvm
  162  nvm install node
  163  node -version
  164  node -v
  165  ls
  166  cd nodejs
  167  vi node.js
  168  npm install express-generator -g
  169  express --view=pug myapp
  170  cd myapp
  171  ls
  172  npm install
  173  npm start
  174  ls
  175  vi app.js
  176  cat public
  177  npm start
  178  ls
  179  cd routes
  180  ls
  181  vi users.js
  182  ls
  183  rm users.js
  184  vi index.js
  185  cd ..
  186  ls
  187  vi app.js
  188  npm install request
  189  npm start
  190  ls
  191  cd routes
  192  ls
  193  vi index.js
  194  cd ..
  195  npm start
  196  npm install -g pm2
  197  pm2
  198  ls
  199  pm2 start app.js
  200  pm2 ls
  201  pm2 moit
  202  pm2 monitor
  203  pm2 moit
  204  pm2 moit
  205  pm2 monit
  206  pm2 ls
  207  pm2 log
  208  pm2 list
  209  pm2 logs
  210  pm2 ls
  211  pm2 delete 0
  212  pm2 ls
  213  ls
  214  pm2 start ./bin/www
  215  pm2 logs
  216  ls
  217  cd bin/
  218  ls
  219  vi www
  220  ls
  221  cd
  222  ls
  223  cd nodejs/
  224  ls
  225  cd myapp/
  226  ls
  227  vi app.js
  228  vi package.json
  229  history | grep pm2

express 설치 후 
app.js 에서 usersRouter 주석
app.user 주석
routes 디렉토리에 index.js에 책임님이 주신 코드로 수정


https://expressjs.com/ko/starter/generator.html
https://velog.io/@mayinjanuary/NVM-%EC%9D%B4%EB%9E%80-%EB%85%B8%EB%93%9CNode.js-%EB%B2%84%EC%A0%84-%EA%B4%80%EB%A6%AC%ED%95%98%EB%8A%94-%EB%B2%95

(설치 및 변경점 추가하기)



```
ubuntu@kwoo-master:~/nodejs/myapp/routes$
```

```
vi index.js
```



```
var express = require('express');
var router = express.Router();
var request = require('request');

router.get('/*',function(req,res,next){

        var request = require('request');
        var options={
                methtod:'GET',
                url:'http://localhost:8001'+req.url,
        };
        console.log(options);
        //console.log(req.body);
        request(options,function(error,response,body){

                //console.log("- body: " + body);
                if(error) throw new Error(error);
        }).pipe(res);

});

router.post('/*',function(req,res,next){

        var request = require('request');
        var data = req.body;
        var options = {
                'method': 'POST',
                'url': 'http://localhost:8001'+req.url,
                'headers': {
                'Content-Type': 'application/json'
                },
  //body: JSON.stringify({"apiVersion":"apps/v1","kind":"Deployment","metadata":{"name":"nginx-deployment","labels":{"app":"nginx"}},"spec":{"replicas":3,"selector":{"matchLabels":{"app":"nginx"}},"template":{"metadata":{"labels":{"app":"nginx"}},"spec":{"containers":[{"name":"ngnix","image":"nginx:1.7.9","ports":[{"containerPort":80}]}]}}}})
                body: JSON.stringify(data),

        };

        console.log(options);
        //console.log(req.body);


request(options, function (error, response) {
  if (error) throw new Error(error);
  //console.log(response.body);
}).pipe(res);
});


router.delete('/*',function(req,res,next){

        var request = require('request');
        var options={
                'method':'DELETE',
                'url':'http://localhost:8001'+req.url,
        };
        console.log(options);
        request(options,function(error,response,body){
                if(error) throw new Error(error);
        }).pipe(res);

});





module.exports = router;
```





-------------
  kubectl run --image=dalgudcks/http-go http-go --port=8080 --dry run -o yaml > http-go-deploy.yaml

  apiVersion: v1
  kind: Service
  metadata:
   name: my-service
  spec:
   selector:
    run: http-go
   ports:
    - protocol: TCP
      port: 80
      targetPort: 8080

  -
  kubectl get all
  kubectl edit http-go-svc

  sessionAffinity: ClientIP 로그인한 파드로만 접근
  -
  kubectl run -it --rm --image=busybox bash

  wget -o- -q 10.8.15.8 (서비스 주소) (로드밸런싱 안댐)
  -
  cloud 환경에서는 external DNS 를 요청

  -
  로드밸런서 기능은 클라우드 환경 부하분산이 존재해야 서비스가 외부 아이피를 받아온다.
  -




------



# Storage

- 볼륨

  - 컨테이너가 외부 스토리지에 액세스하고 공유하는 방법

  - 포드의 각 컨테이너에는 고유의 분리된 파일 시스템이 존재

  - 볼륨은 포드의 컴포넌트이며 포드의 스펙에 의해 정의

  - 독립적인 쿠버네티스 오브젝트가 아니며 스스로 생성, 삭제 불가

  - 각 컨테이너의 파일 시스템의 볼륨을 마운트하여 생성

  - 볼륨의 종류

    | 임시 볼륨 | 로컬 볼륨      | 네트워크 볼륨                  | 네트워크 볼륨 (클라우드 종속적)        |
    | --------- | -------------- | ------------------------------ | -------------------------------------- |
    | emptyDir  | hostpath local | iSCSI NFS cephFS glusterFS ... | gcePersistentDisk awsEBS azureFile ... |




- 주요 사용 가능한 볼륨의 유형
  - emptyDir: 일시적인 데이터 저장, 비어 있는 디렉터리 (컨테이너 파괴 시 같이 파괴 컨테이너끼리 데이터 공유 하도록 만듬)
  - hostPath local: 포드에 호스트 노드의 파일 시스템에서 파일이나 디렉토리를 마운트 (pod가 위치한 노드랑 filesystem을 공유한다.)
  - nfs: 기존 NFS (네트워크 파일 시스템) 공유가 포드에 장착
  - gcePersistentDisk: 구글 컴퓨트 엔진(GCE) 영구디스크 마운트
  - persistentVolumeClaim: 사용자가 특정 클라우드 환경의 세부 사항을 모른 채 GCE PersistentDIsk 또는 iSCSI 볼륨과 같은 내구성 스토리지를 요구(Claim) 할 수 있는 방법
  - configMap, Secret, downwardAPI: 특수한 유형의 볼륨
- https://kubernetes.io/docs/concepts/storage/volumes/#persistentvolumeclaim



- Hostpath Local
  - 노드가 달라지면 data가 달라진다.
  - 노드의 directory를 나에게 mount한다



- 임시 볼륨
  - 컨테이너 공유
- local 볼륨
  - 노드와 공유
- 네트워크 볼륨
  - 클러스터 외부에 있는 자원과 데이터 공유
- HostPath 볼륨
  - 컨테이너 - 노드 간의 공유
  - 주로 모니터링 용도로 많이 씀
  - 로그 수집기에서 주로 사용
  - 내부의 filesystem 공유 
- fluentd -
  - 외부에서 모니터링 데이터를 수집하는 도구



- emptyDir 볼륨 작성하기
  - 볼륨을 공유하는 애플리케이션 생성
  - docker build -t dalgudcks/count .
  - docker push dalgudcks/count



- 





# 데몬셋

​       노드당 포드 하나씩

- 레플리케이션 컨트롤러와 레플리카셋은 무작위 노드에 포드를 생성
- 데몬셋은 각 하나의 노드에 하나의 포드만을 구성
- kube-proxy가 데몬셋으로 만든 쿠버네티스에서 기본적으로 활동중인 포드



- *데몬셋* 은 모든(또는 일부) 노드가 파드의 사본을 실행하도록 한다. 노드가 클러스터에 추가되면 파드도 추가된다. 노드가 클러스터에서 제거되면 해당 파드는 가비지(garbage)로 수집된다. 데몬셋을 삭제하면 데몬셋이 생성한 파드들이 정리된다.

- 다른 모든 쿠버네티스 설정과 마찬가지로 데몬셋에는 `apiVersion`, `kind` 그리고 `metadata` 필드가 필요하다. 



필드 참고

https://kubernetes.io/ko/docs/concepts/workloads/controllers/daemonset/



```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: kube-system
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      tolerations:
      # this toleration is to have the daemonset runnable on master nodes
      # remove it if your masters can't run pods
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers

```



# ISTIO 



### 서비스 메쉬란

- API 등을 이용하여 마이크로 서비스 간 통신을 안전하고 빠르고 신뢰할 수 있게 만들기 위해 설꼐된 전용 인프라 계층
- 서비스 메쉬는 보통 Application 서비스에 경량화 프록시를 사이드카 방식으로 배치하여 서비스 간 통신을 제어한다.

### 서비스 메쉬가 필요한 이유

- 시스템 규모에 따라 다르지만, MSA 시스템에서 수십개의 마이크로 서비스가 동작할 수 있고 그보다 많은 인스턴스들이 동작할 수 있기 때문에 관리하기가 매우 복잡하다. 또한 인스턴스가 수행되는 네트워크 간의 레이턴시,신뢰성,안전성 등을 보장 할 수 없다.
- 이러한 문제점들은 어플리케이션 계층에서 해결할 수도 있지만, 그렇게 되면 Application 언어 및 런타임에 종속성이 생기고, 기능 변경 등을 할 떄마다 비용이 발생한다.
- 그렇기 때문에 마이크로 서비스 운영할 때에는 별도의 소스 수정이 필요없이 인프라 레벨에서 안정적으로 관리할 수 있는 서비스 메쉬가 필요하다.



### Istio 정의

- Istio는 서비스 메쉬를 구현할 수 있는 오픈소스 솔루션

- 마이크로 서비스 간의 모든 네트워크 통신을 담당할 수 있는 프록시인 Envoy를 사이드카 패턴으로 마이크로 서비스들에 배포한 다음, 프록시들의 설정값 저장 및 관리/감독을 수행하고, 프록시들에 설정값을 전달하는 컨트롤러 역할 수행

- 각각의 마이크로 서비스에 사이드카 패턴으로 배포된 Envoy 프록시를 데이타 플레인이라 함
- 데이타 플레인을 컨트롤 하는 부분은 컨트롤 플레인이라고 함



### 구성요소

1. 데이타 플레인



2. 컨트롤 플레인



### 주요 특징



포드 안에 프록시를 넣는다 프록시를 거쳐서 통신함 (속도 느림)

포드에 있는 정보를 수집해서 istio로 확인한다. 통신 확인



서비스의 코드를 변경하지 않고도 로드밸런싱, 서비스 간 인증, 모니터링 등을 통해 배포된 서비스 네트워크를 쉽게 생성 가능하다는 점

레이블만 바꿔주면 알아서 프록시에서 자동으로 바뀐다.



```
istioctl profile list
```









----

해석

```
Deployment Models

When configuring a production deployment of Istio, you need to answer a number of questions. Will the mesh be confined to a single cluster or distributed across multiple clusters? Will all the services be located in a single fully connected network, or will gateways be required to connect services across multiple networks? Is there a single control plane, potentially shared across clusters, or are there multiple control planes deployed to ensure high availability (HA)? Are all clusters going to be connected into a single multicluster service mesh or will they be federated into a multi-mesh deployment?

All of these questions, among others, represent independent dimensions of configuration for an Istio deployment.

single or multiple cluster
single or multiple network
single or multiple control plane
single or multiple mesh

In a production environment involving multiple clusters, you can use a mix of deployment models. For example, having more than one control plane is recommended for HA, but you could achieve this for a 3 cluster deployment by deploying 2 clusters with a single shared control plane and then adding the third cluster with a second control plane in a different network. All three clusters could then be configured to share both control planes so that all the clusters have 2 sources of control to ensure HA.

Choosing the right deployment model depends on the isolation, performance, and HA requirements for your use case. This guide describes the various options and considerations when configuring your Istio deployment.


```

```
Cluster models

The workload instances of your application run in one or more clusters. For isolation, performance, and high availability, you can confine clusters to availability zones and regions.
Production systems, depending on their requirements, can run across multiple clusters spanning a number of zones or regions, leveraging cloud load balancers to handle things like locality and zonal or regional fail over.

In most cases, clusters represent boundaries for configuration and endpoint discovery. For example, each Kubernetes cluster has an API Server which manages the configuration for the cluster as well as serving service endpoint information as pods are brought up or down. Since Kubernetes configures this behavior on a per-cluster basis, this approach helps limit the potential problems caused by incorrect configurations.
In Istio, you can configure a single service mesh to span any number of clusters.
```

```
Single cluster

In the simplest case, you can confine an Istio mesh to a single cluster. A cluster usually operates over a single network, but it varies between infrastructure providers. A single cluster and single network model includes a control plane, which results in the simplest Istio deployment.

Single cluster deployments offer simplicity, but lack other features, for example, fault isolation and fail over. If you need higher availability, you should use multiple clusters.
```

```
Multiple clusters

You can configure a single mesh to include multiple clusters. Using a multicluster deployment within a single mesh affords the following capabilities beyond that of a single cluster deployment:

- Fault isolation and fail over: cluster-1 goes down, fail over to cluster-2.
- Location-aware routing and fail over: Send requests to the nearest service.
- Various control plane models: Support different levels of availability.
- Team or project isolation: Each team runs its own set of clusters.


```

