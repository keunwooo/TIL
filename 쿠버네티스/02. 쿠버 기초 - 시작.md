# 쿠버 기초



쿠버네티스 1.18버전 업데이트로 인해 kubectl 명령이 변경됨

kubectl run -> kubectl create deploy 로 변경 --port 빼고

--dry-run 명령은 --dry-run=client

```
kubectl create configmap my-configmap --from-literal mykey=myvalue --dry-run -o yaml
```



https://arisu1000.tistory.com/27834

쿠버네티스 리셋

```
sudo kubeadm reset -f
sudo rm -rf /etc/cni /etc/kubernetes /var/lib/dockershim /var/lib/etcd /var/lib/kubelet /var/run/kubernetes ~/.kube/*
```

재실행

```
systemctl restart kubelet
```



1. 마스터 노드

   - 설정하기

   ```
   vi install.sh
   
   sudo apt-get update && sudo apt-get install -y apt-transport-https curl
   curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
   cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
   deb https://apt.kubernetes.io/ kubernetes-xenial main
   EOF
   sudo apt-get update
   sudo apt-get install -y kubelet kubeadm kubectl
   sudo apt-mark hold kubelet kubeadm kubectl
   
   저장 후 
   
   bash install.sh
   
   설치 후
   
   sudo kubeadm init
   sudo swapoff -a
   sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab 
   
   노드 초기화 후 나오는 메세지 중 아래 실행
   
   mkdir -p $HOME/.kube
   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
   sudo chown $(id -u):$(id -g) $HOME/.kube/config
   
   중요! Pod Network 추가
   kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
   
   kubeadm join 10.0.0.178:6443 --token 1insnt.r880f4ingbwjjcmg \
       --discovery-token-ca-cert-hash sha256:8999b49f36a2c9a30f0ecf95c4bbb78bd2562b4f189924b62def3995b829912d
   ```



마스터노드 install.sh

```
sudo apt-get update
sudo apt install docker.io
sudo apt-get update && sudo apt-get install -y apt-transport-https curl
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
sudo kubeadm init
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"

kubectl delete ns metallb-system
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.5/manifests/namespace.yaml
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.5/manifests/metallb.yaml
kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey="$(openssl rand -base64 128)"

echo "------install metalLB Ok---------"

kubectl apply -f config.yaml

echo "------config.yaml Ok---------"

```

```
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config:
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - 10.0.0.225/32
```



워커노드 install.sh

```
sudo apt-get update
sudo apt install docker.io
sudo apt-get update && sudo apt-get install -y apt-transport-https curl
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

```



쉘 다시 작성중

https://gist.github.com/keunwooo/791701c4867bcd0c45ebf2ef5f00143f 확인



submarnier 를 위한 코드 확인

```
my_vm_internal_ip="$(hostname -I | awk {'print $1'})"

# broker
calico_pod_cidr="10.240.0.0/16"
calico_network_cidr="10.110.0.0/16"
instance_public_ip="118.130.73.14"

kubeadm init --v=5 --pod-network-cidr=${calico_pod_cidr}  --service-cidr=${calico_network_cidr} --apiserver-cert-extra-sans "${my_vm_internal_ip},${instance_public_ip}"
```

---



1. 슬레이브 노드 

   - 설정하기

   ```
   1. 도커를 설치한다
   sudo apt-get update
   sudo apt install docker.io
   
   2. 마스터 노드에서 쿠버네티스를 설치한 것과 동일하게 설치한다.
   3. init 명령어 전까지만 수행한다.
   
   마스터에서 init 후 나온 다음과 같은 메시지로 워커 노드를 참가 시킨다.
   
   sudo kubeadm join 10.0.2.15:6443 --token xwvbff.5xc67j8qc6ohl2it \
   --discovery-token-ca-cert-hash
   sha256:e19e9263aeb2340a602c2057966b71551e01a5e287d3f23b05073c7b248932e1
   ```

   

2. 쿠버네티스 명령어

   - 연결된 노드들의 상태를 확인

   ```
   kubectl get nodes
   ```

   - 간단한 애플리케이션 실행 및 확인

   ```
   kubectl create deploy --image=nginx
   kubectl get pod
   kubectl port-forward nginx-XXXXXXXXXX-XXXXX 8080:80
   ```

   - 포드 확인

   ```
   kubectl get pods
   ```

   - 서비스 확인

   ```
   kubectl get services
   ```

   - 애플리케이션 수평 스케일링

   ```
    kubectl scale deploy http-go --replicas=3
   ```

   - 디플로이 확인

   ```
    kubectl get deploy
   ```

   - 직접 앱에 접근하기

   ```
   kubectl get pods 로 아이피 확인
   kubectl exec http-go-XXXXXX-bt4xq -- curl -s http://10.109.140.155:8080
   ```

   - 앱의 위치 확인

   ```
   kubectl get pod -o wide
   ```

   - 포드의 자세한 내용 살펴보기

   ```
   kubectl describe pod http-go-XXXXXX-bt4xq
   ```

   - 모든 서비스 지우기

   ```
   kubectl delete all --all
   ```

   - 쿠버니테스 주요 컴포넌트 확인하기

   ```
    kubectl get pod -n kube-system
   ```

   - 라벨 함께 출력

   ```
   kubectl get pods --show-labels
   ```

   - 디플로이먼트에서 생성된 포드의 이미지를 변경

   ```
   kubectl set image deployment my-nginx-deployment nginx=nginx:1.11 --record
   
   --record 옵션으로 이전 리비전 정보 생성
   
   nginx라는 이름을 가지는 컨테이너의 이미지를 nginx:1.11으로 변경
   
   이미지를 업데이트하면 레플리카셋이 새롭게 생성됨
   
   이전 버전의 레플리카셋을 삭제하지 않고 남겨둠
   
   디플로이먼트는 포드의 정보가 변경되어 업데이트가 발생하면, 이전의 정보를 리비전으로서 보존한다.
   ```

   - 리비전 정보 조회

   ```
   kubectl rollout history deployment my-nginx-deployment
   ```

   - 이전 버전의 레플리카셋으로 롤백

   ```
   kubectl rollout undo deployment my-nginx-deployment --to-revision=1
   
   되돌리려는 리비전 번호 입력
   ```

   - 





1. 쿠버네티스에 앱 실행해보기

   - go 언어로 main.go 작성

   ```
   package main
   
   import (
           "fmt"
           "github.com/julienschmidt/httprouter"
           "net/http"
           "log"
           "os"
   )
   
   func Index(w http.ResponseWriter, r *http.Request, _ httprouter.Params){
           hostname,err:=os.Hostname()
           if err == nil {
                   fmt.Fprint(w,"Welcome! " + hostname +"\n")
           } else{
                   fmt.Fprint(w, "Welcome! Error\n")
           }
   }
   
   func main() {
           router := httprouter.New()
           router.GET("/",Index)
   
           log.Fatal(http.ListenAndServe(":8080",router))
   }
   ```

   - Go 언어 설치 및 빌드

   ```
   apt install golang
   go get github.com/julienschmidt/httprouter
   go build main.go
   
   ./main
   
   외부에서 접속해서 확인
   
   ```

   - dockerfile 작성

   ```
   FROM golang:1.11
   WORKDIR /usr/src/app
   COPY main /usr/src/app
   CMD ["/usr/src/app/main"]
   ```

   - 컨테이너 이미지 만들기

   ```
   docker build -t http-go
   ```

   - 컨테이너 실행해서 확인

   ```
   docker run -d -p 8080:8080 --rm http-go
   ```

   - 도커허브에 컨테이너 푸시하기

   ```
   docker tag http-go dalgudcks/http-go
   docker login
   
   docker push dalgudcks/http-go
   ```

   - 명령어에 몇 가지 옵션으로 디스크립션을 간단히 전달하여 한줄로 앱을 실행

   ```
    kubectl create deploy http-go --image=dalgudcks/http-go
   ```

   - 로드밸런서라는 서비스를 작성하여 외부 로드 밸런서를 생성한다. 

   ```
    kubectl expose deployment http-go --type=LoadBalancer --name http-go-svc --port=8080 --target-port=8080
   
   kubectl get services
   ```

- 앱에 접근하기
```
   kubectl exec http-go-XXXXXX-bt4xq -- curl -s http://10.109.140.155:8080
   
   curl 명령어로 요청
   external IP를 할당 받지 못했기 때문에 포드의 힘을 빌려 요청한다.
```



kube-system의 역할

### etcd



----

해석

```
Deployment Models

When configuring a production deployment of Istio, you need to answer a number of questions. Will the mesh be confined to a single cluster or distributed across multiple clusters? Will all the services be located in a single fully connected network, or will gateways be required to connect services across multiple networks? Is there a single control plane, potentially shared across clusters, or are there multiple control planes deployed to ensure high availability (HA)? Are all clusters going to be connected into a single multicluster service mesh or will they be federated into a multi-mesh deployment?

All of these questions, among others, represent independent dimensions of configuration for an Istio deployment.

single or multiple cluster
single or multiple network
single or multiple control plane
single or multiple mesh

In a production environment involving multiple clusters, you can use a mix of deployment models. For example, having more than one control plane is recommended for HA, but you could achieve this for a 3 cluster deployment by deploying 2 clusters with a single shared control plane and then adding the third cluster with a second control plane in a different network. All three clusters could then be configured to share both control planes so that all the clusters have 2 sources of control to ensure HA.

Choosing the right deployment model depends on the isolation, performance, and HA requirements for your use case. This guide describes the various options and considerations when configuring your Istio deployment.


```

```
Cluster models

The workload instances of your application run in one or more clusters. For isolation, performance, and high availability, you can confine clusters to availability zones and regions.
Production systems, depending on their requirements, can run across multiple clusters spanning a number of zones or regions, leveraging cloud load balancers to handle things like locality and zonal or regional fail over.

In most cases, clusters represent boundaries for configuration and endpoint discovery. For example, each Kubernetes cluster has an API Server which manages the configuration for the cluster as well as serving service endpoint information as pods are brought up or down. Since Kubernetes configures this behavior on a per-cluster basis, this approach helps limit the potential problems caused by incorrect configurations.
In Istio, you can configure a single service mesh to span any number of clusters.
```

```
Single cluster

In the simplest case, you can confine an Istio mesh to a single cluster. A cluster usually operates over a single network, but it varies between infrastructure providers. A single cluster and single network model includes a control plane, which results in the simplest Istio deployment.

Single cluster deployments offer simplicity, but lack other features, for example, fault isolation and fail over. If you need higher availability, you should use multiple clusters.
```

```
Multiple clusters

You can configure a single mesh to include multiple clusters. Using a multicluster deployment within a single mesh affords the following capabilities beyond that of a single cluster deployment:

- Fault isolation and fail over: cluster-1 goes down, fail over to cluster-2.
- Location-aware routing and fail over: Send requests to the nearest service.
- Various control plane models: Support different levels of availability.
- Team or project isolation: Each team runs its own set of clusters.


```



---

