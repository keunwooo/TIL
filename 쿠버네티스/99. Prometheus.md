# Prometheus



### Storage

Prometheus의 로컬 시계열 데이터베이스는 로컬 저장소에 사용자 지정의 매우 효율적인 형식으로 데이터를 저장합니다.

수집 된 샘플은 2 시간 단위로 그룹화됩니다. 각 2 시간 블록은 해당 기간에 대한 모든 시계열 샘플을 포함하는 하나 이상의 청크 파일과 메타 데이터 파일 및 인덱스 파일 (측정 항목 이름과 레이블을 청크 파일의 시계열로 인덱싱하는)이 포함 된 디렉토리로 구성됩니다. ). API를 통해 시리즈를 삭제하면 삭제 기록이 별도의 삭제 표시 파일에 저장됩니다 (청크 파일에서 데이터를 즉시 삭제하는 대신).

수신 샘플의 현재 블록은 메모리에 보관되며 완전히 유지되지 않습니다. Prometheus 서버가 다시 시작될 때 재생할 수있는 미리 쓰기 로그 (WAL)를 통해 충돌로부터 보호됩니다. 미리 쓰기 로그 파일은 128MB 세그먼트로 wal 디렉토리에 저장됩니다. 이러한 파일에는 아직 압축되지 않은 원시 데이터가 포함되어 있습니다. 따라서 일반 블록 파일보다 훨씬 큽니다. Prometheus는 최소 3 개의 미리 쓰기 로그 파일을 유지합니다. 트래픽이 많은 서버는 최소 2 시간의 원시 데이터를 보관하기 위해 3 개 이상의 WAL 파일을 보관할 수 있습니다.



로컬 저장소의 한계는 클러스터되거나 복제되지 않는다는 것입니다. 따라서 드라이브 또는 노드 중단시 임의로 확장 가능하거나 내구성이 없으며 다른 단일 노드 데이터베이스처럼 관리해야합니다. 스토리지 가용성을 위해 RAID 사용을 권장하고 백업에는 스냅 샷을 권장합니다. 적절한 아키텍처를 사용하면 로컬 스토리지에 수년간의 데이터를 보관할 수 있습니다.



- `--storage.tsdb.path`: Where Prometheus writes its database. Defaults to `data/`.
- `--storage.tsdb.retention.time`: When to remove old data. Defaults to `15d`. Overrides `storage.tsdb.retention` if this flag is set to anything other than default.
- `--storage.tsdb.retention.size`: [EXPERIMENTAL] The maximum number of bytes of storage blocks to retain. The oldest data will be removed first. Defaults to `0` or disabled. This flag is experimental and may change in future releases. Units supported: B, KB, MB, GB, TB, PB, EB. Ex: "512MB"
- `--storage.tsdb.retention`: Deprecated in favor of `storage.tsdb.retention.time`.
- `--storage.tsdb.wal-compression`: Enables compression of the write-ahead log (WAL). Depending on your data, you can expect the WAL size to be halved with little extra cpu load. This flag was introduced in 2.11.0 and enabled by default in 2.20.0. Note that once enabled, downgrading Prometheus to a version below 2.11.0 will require deleting the WAL.



Prometheus는 다음 세 가지 방법으로 원격 스토리지 시스템과 통합됩니다

Prometheus는 표준화 된 형식으로 원격 URL에 수집하는 샘플을 작성할 수 있습니다.
Prometheus는 표준화 된 형식으로 다른 Prometheus 서버에서 샘플을받을 수 있습니다.
Prometheus는 표준화 된 형식으로 원격 URL에서 샘플 데이터를 읽을 수 있습니다.



### TSDB(Time-series Database)

이렇게 가져온 데이터를 저장하고, 시간의 흐름에 따라 조회를 할 수 있어야 하므로 시-계열 데이터(time-series) 를 저장할 수 있는 저장소가 prometheus 내부에 구현이 되어 있다.

데이터를 저장하는 방법은 Local Storage, Remote Storage를 이용하는 방법 두 가지가 존재한다. 대부분 Local Storage를 쓰는걸로 default로 사용하겠지만 필요에 따라서 원격지에 있는 서버에 데이터를 저장해서 사용을 한다고 한다



### Concept

- Data Model
  - Metric name
    - 측정되는 시스템의 기능
    - ex) http_requests_total
  - Label
    - Metric name에 대응되는 다수의 측정 항목
    - key와 value로 이루어짐
    - ex) method=”POST”, handler=”/messages”
    - Influx의 tag에 해당
- Job And Instances
  - Instance
    - 스크랩할수 있는 엔드포인트 (수집을 당하는 곳)
    - 보통 단일 프로세스임
  - Job
    - 확장성이나 안정성을 위해 복제된 동일한 인스턴스의 모음
  - Example
    - job: api-server
      - instance 1: 1.2.3.4:5670
      - instance 2: 1.2.3.4:5671
      - instance 3: 5.6.7.8:5670
- Recoding rules
  - Influx의 Continous Query와 같은 역할을 한다.
  - 이미 존재하는 Metric으로 새로운 Time Series Recode를 만들어낸다.
  - ex) latency를 5초 동안 평균을 낸다.
- Storage
  - 기본적으로 로컬 on-disk 시계열 DB를 포함하고 있음
  - 그러나 remote storage도 허용
  - Local Stroage
    - 샘플들은 두시간 단위의 블록들로 그룹되어짐
    - 들어오고 있는 데이터들은 메모리상에서 유지
    - 그러나 WAL(write ahead log) 방식으로 장애 복구시 메모리에서 유지됐던것을 복구 가능
    - tsdb 포맷을 사용하며 블록(디렉토리) 안에는 다음의 내용들로 구성됨
      - meta.json (메타 데이터)
      - wal (write ahead log)
      - chunks (청크 데이터)
      - tombstones (삭제 표시) - 데이터 삭제시 데이터를 바로 삭제하지 않고 표시해둠
    - 다음의 중요한 설정 가능
      - –storage.tsdb.path
        - 데이터를 저장할 디렉토리를 지정
        - default : ./data
      - –storage.tsdb.retention
        - 언제까지 디스크에 남겨놓을건지?
        - 디스크 크기를 구하는 공식
          - needed_disk_space = retention_time_seconds * ingested_samples_per_second * bytes_per_sample
- FEDERATION
  - 확장 가능한 Prometheus 모니터링 설정을 달성하거나
  - 특정 서비스의 Prometheus에서 관련 메트릭을 다른 것으로 가져 오는데 사용
  - 종류
    - Hierarchical Federation
    - Cross-service Ferderation
- Exporters
  - 모니터링 대상의 Metric 정보를 수집
  - Exporter Http EndPoint를 통해 Prometheus에서 Metric을 수집 (Pull 방식)
  - 서드 파티 시스템에서 Prometheus metrics를 export하기 위해 사용
- AlertManager
  - Prometheus Server와 같은 클라이언트 어플리케이션에 의해 보내진 Alert을 처리
  - 다음 종류의 Alert이 있음
    - Grouping
      - 비슷한 성격의 경고를 단일 알람으로 분류함
      - ex) 수백개의 인스턴스에서 발생하는 Alert을 하나의 그룹으로 묶어 단일 Alert으로 전송
    - Inhibition
      - 특정 다른 Alert이 이미 발생하면, 특정 Alert을 억제함
      - 클러스터안의 전체 인스턴스에 접근이 불가능할 경우 이 클러스터와 관련된 다른 모든 경고를 음소거하도록함
      - 상관없는 다른 Alert들을 억제함
    - Silences
      - 주어진 시간 동안 단순히 Alert을 음소거함



---

쿠버네티스 클러스터에 프로메테우스 모니터링을 적용하기 위해선 다음과 같은 리소스가 필요하다.

- cluster-role : 프로메테우스 컨테이너가 쿠버네티스 api에 접근할 수 있는 권한을 부여한다. 
- config-map : 프로메테우스가 기동되려면 프로메테우스 환경설정파일(prometheus.rules, prometheus.yaml)이 필요한데, 해당 환경설정파일을 정의해줌. 프로메테우스 컨테이너가 기동되면 config-map에 정의된 prometheus.yaml이 컨테이너 내부로 들어가게 된다. prometheus.rules에는 수집한 지표에 대한 알람조건을 지정하여 특정 조건이 되면 prometheus에서 AlertManager로 알람을 보낼 수 있고, prometheus.yaml에는 수집할 지표(Metric) 종류, 지표 수집 주기 등을 기입한다. 
- deployment : 프로메테우스 deployment로 pod 가 구동될 때 필요하다.
- service : 컨테이너 외부에서 프로메테우스로 접근하기 위해 필요하다.
- daemonset(node-exporter) : 쿠버네티스 클러스터 정보를 수집하기 위해 node-exporter가 필요하며 해당 exporter는 각 노드당 1개씩만 올라가기 때문에 daemonset 타입으로 구동시킨다.







1. 클러스터 생성
2. Cluster Role

다음으로는 프로메테우스 컨테이너가 **쿠버네티스 api에 접근할 수 있는 권한**을 부여해주기 위해 `ClusterRole`을 설정해주고 `ClusterRoleBinding`을 해줍니다.
생성된 ClusterRole은 monitoring namespace의 기본 서비스어카운트와 연동되어 권한을 부여해줍니다.

```
prometheus-cluster-role.yaml

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: prometheus
  namespace: monitoring
rules:
- apiGroups: [""]
  resources:
  - nodes
  - nodes/proxy
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups:
  - extensions
  resources:
  - ingresses
  verbs: ["get", "list", "watch"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: default
  namespace: monitoring
```

3. ### Configmap

프로메테우스가 기동되려면 환경설정파일이 필요한데 해당 환경설정 파일을 정의해주는 부분입니다.

data 밑에 `prometheus.rules`와 `prometheus.yml`를 각각 정의하게 되어있습니다.

- **prometheus.rules** : 수집한 지표에 대한 알람조건을 지정하여 특정 조건이 되면 AlertManager로 알람을 보낼 수 있음
- **prometheus.yml** : 수집할 지표(metric)의 종류와 수집 주기등을 기입

```
prometheus-config-map.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-server-conf
  labels:
    name: prometheus-server-conf
  namespace: monitoring
data:
  prometheus.rules: |-
    groups:
    - name: container memory alert
      rules:
      - alert: container memory usage rate is very high( > 55%)
        expr: sum(container_memory_working_set_bytes{pod!="", name=""})/ sum (kube_node_status_allocatable_memory_bytes) * 100 > 55
        for: 1m
        labels:
          severity: fatal
        annotations:
          summary: High Memory Usage on 
          identifier: ""
          description: " Memory Usage: "
    - name: container CPU alert
      rules:
      - alert: container CPU usage rate is very high( > 10%)
        expr: sum (rate (container_cpu_usage_seconds_total{pod!=""}[1m])) / sum (machine_cpu_cores) * 100 > 10
        for: 1m
        labels:
          severity: fatal
        annotations:
          summary: High Cpu Usage
  prometheus.yml: |-
    global:
      scrape_interval: 5s
      evaluation_interval: 5s
    rule_files:
      - /etc/prometheus/prometheus.rules
    alerting:
      alertmanagers:
      - scheme: http
        static_configs:
        - targets:
          - "alertmanager.monitoring.svc:9093"

    scrape_configs:
      - job_name: 'kubernetes-apiservers'

        kubernetes_sd_configs:
        - role: endpoints
        scheme: https

        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        relabel_configs:
        - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
          action: keep
          regex: default;kubernetes;https

      - job_name: 'kubernetes-nodes'

        scheme: https

        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        kubernetes_sd_configs:
        - role: node

        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics


      - job_name: 'kubernetes-pods'

        kubernetes_sd_configs:
        - role: pod

        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: kubernetes_pod_name

      - job_name: 'kube-state-metrics'
        static_configs:
          - targets: ['kube-state-metrics.kube-system.svc.cluster.local:8080']

      - job_name: 'kubernetes-cadvisor'

        scheme: https

        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        kubernetes_sd_configs:
        - role: node

        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor

      - job_name: 'kubernetes-service-endpoints'

        kubernetes_sd_configs:
        - role: endpoints

        relabel_configs:
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
          action: replace
          target_label: __scheme__
          regex: (https?)
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
          action: replace
          target_label: __address__
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
        - action: labelmap
          regex: __meta_kubernetes_service_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_service_name]
          action: replace
          target_label: kubernetes_name
```



4. deployment

프로메테우스 이미지를 담은 pod을 담은 deployment controller

```
prometheus-deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus-deployment
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus-server
  template:
    metadata:
      labels:
        app: prometheus-server
    spec:
      containers:
        - name: prometheus
          image: prom/prometheus:latest
          args:
            - "--config.file=/etc/prometheus/prometheus.yml"
            - "--storage.tsdb.path=/prometheus/"
          ports:
            - containerPort: 9090
          volumeMounts:
            - name: prometheus-config-volume
              mountPath: /etc/prometheus/
            - name: prometheus-storage-volume
              mountPath: /prometheus/
      volumes:
        - name: prometheus-config-volume
          configMap:
            defaultMode: 420
            name: prometheus-server-conf

        - name: prometheus-storage-volume
          emptyDir: {}
```



5. node exporter

```
prometheus-node-exporter.yaml

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: monitoring
  labels:
    k8s-app: node-exporter
spec:
  selector:
    matchLabels:
      k8s-app: node-exporter
  template:
    metadata:
      labels:
        k8s-app: node-exporter
    spec:
      containers:
      - image: prom/node-exporter
        name: node-exporter
        ports:
        - containerPort: 9100
          protocol: TCP
          name: http
---
apiVersion: v1
kind: Service
metadata:
  labels:
    k8s-app: node-exporter
  name: node-exporter
  namespace: kube-system
spec:
  ports:
  - name: http
    port: 9100
    nodePort: 31672
    protocol: TCP
  type: NodePort
  selector:
    k8s-app: node-exporter
```



6. Service

```
prometheus-svc.yaml

apiVersion: v1
kind: Service
metadata:
  name: prometheus-service
  namespace: monitoring
  annotations:
      prometheus.io/scrape: 'true'
      prometheus.io/port:   '9090'
spec:
  selector:
    app: prometheus-server
  type: NodePort
  ports:
    - port: 8080
      targetPort: 9090
      nodePort: 30003
```

7. 배포

```
kubectl apply -f prometheus-cluster-role.yaml
kubectl apply -f prometheus-config-map.yaml
kubectl apply -f prometheus-deployment.yaml
kubectl apply -f prometheus-node-exporter.yaml
kubectl apply -f prometheus-svc.yaml

kubectl get pod -n monitoring
```



8. ### Kube State Metrics 배포

```
kube-state-cluster-role.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kube-state-metrics
subjects:
- kind: ServiceAccount
  name: kube-state-metrics
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: kube-state-metrics
rules:
- apiGroups:
  - ""
  resources: ["configmaps", "secrets", "nodes", "pods", "services", "resourcequotas", "replicationcontrollers", "limitranges", "persistentvolumeclaims", "persistentvolumes", "namespaces", "endpoints"]
  verbs: ["list","watch"]
- apiGroups:
  - extensions
  resources: ["daemonsets", "deployments", "replicasets", "ingresses"]
  verbs: ["list", "watch"]
- apiGroups:
  - apps
  resources: ["statefulsets", "daemonsets", "deployments", "replicasets"]
  verbs: ["list", "watch"]
- apiGroups:
  - batch
  resources: ["cronjobs", "jobs"]
  verbs: ["list", "watch"]
- apiGroups:
  - autoscaling
  resources: ["horizontalpodautoscalers"]
  verbs: ["list", "watch"]
- apiGroups:
  - authentication.k8s.io
  resources: ["tokenreviews"]
  verbs: ["create"]
- apiGroups:
  - authorization.k8s.io
  resources: ["subjectaccessreviews"]
  verbs: ["create"]
- apiGroups:
  - policy
  resources: ["poddisruptionbudgets"]
  verbs: ["list", "watch"]
- apiGroups:
  - certificates.k8s.io
  resources: ["certificatesigningrequests"]
  verbs: ["list", "watch"]
- apiGroups:
  - storage.k8s.io
  resources: ["storageclasses", "volumeattachments"]
  verbs: ["list", "watch"]
- apiGroups:
  - admissionregistration.k8s.io
  resources: ["mutatingwebhookconfigurations", "validatingwebhookconfigurations"]
  verbs: ["list", "watch"]
- apiGroups:
  - networking.k8s.io
  resources: ["networkpolicies"]
  verbs: ["list", "watch"]
```



9.  Cluster  Role과 연동될 서비스어카운트 생성

```
kube-state-svcaccount.yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  name: kube-state-metrics
  namespace: kube-system
```



10. kube-state-metrics 의 deployment 구성

```
kube-state-deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: kube-state-metrics
  name: kube-state-metrics
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kube-state-metrics
  template:
    metadata:
      labels:
        app: kube-state-metrics
    spec:
      containers:
      - image: quay.io/coreos/kube-state-metrics:v1.8.0
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 5
        name: kube-state-metrics
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 8081
          name: telemetry
        readinessProbe:
          httpGet:
            path: /
            port: 8081
          initialDelaySeconds: 5
          timeoutSeconds: 5
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: kube-state-metrics
```



11. kube-state-metrics 서비스 생성

```
kube-state-svc.yaml

apiVersion: v1
kind: Service
metadata:
  labels:
    app: kube-state-metrics
  name: kube-state-metrics
  namespace: kube-system
spec:
  clusterIP: None
  ports:
  - name: http-metrics
    port: 8080
    targetPort: http-metrics
  - name: telemetry
    port: 8081
    targetPort: telemetry
  selector:
    app: kube-state-metrics
```



12. 배포

```
$ kubectl apply -f kube-state-cluster-role.yaml
$ kubectl apply -f kube-state-deployment.yaml
$ kubectl apply -f kube-state-svcaccount.yaml
$ kubectl apply -f kube-state-svc.yaml

kubectl get pod -n kube-system
```



```
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: prometheus
  namespace: monitoring
rules:
- apiGroups: [""]
  resources:
  - nodes
  - nodes/proxy
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups:
  - extensions
  resources:
  - ingresses
  verbs: ["get", "list", "watch"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: default
  namespace: monitoring

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-server-conf
  labels:
    name: prometheus-server-conf
  namespace: monitoring
data:
  prometheus.rules: |-
    groups:
    - name: container memory alert
      rules:
      - alert: container memory usage rate is very high( > 55%)
        expr: sum(container_memory_working_set_bytes{pod!="", name=""})/ sum (kube_node_status_allocatable_memory_bytes) * 100 > 55
        for: 1m
        labels:
          severity: fatal
        annotations:
          summary: High Memory Usage on
          identifier: ""
          description: " Memory Usage: "
    - name: container CPU alert
      rules:
      - alert: container CPU usage rate is very high( > 10%)
        expr: sum (rate (container_cpu_usage_seconds_total{pod!=""}[1m])) / sum (machine_cpu_cores) * 100 > 10
        for: 1m
        labels:
          severity: fatal
        annotations:
          summary: High Cpu Usage
  prometheus.yml: |-
    global:
      scrape_interval: 5s
      evaluation_interval: 5s
    rule_files:
      - /etc/prometheus/prometheus.rules
    alerting:
      alertmanagers:
      - scheme: http
        static_configs:
        - targets:
          - "alertmanager.monitoring.svc:9093"

    scrape_configs:
      - job_name: 'kubernetes-apiservers'

        kubernetes_sd_configs:
        - role: endpoints
        scheme: https

        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        relabel_configs:
        - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
          action: keep
          regex: default;kubernetes;https

      - job_name: 'kubernetes-nodes'

        scheme: https

        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        kubernetes_sd_configs:
        - role: node

        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics


      - job_name: 'kubernetes-pods'

        kubernetes_sd_configs:
        - role: pod

        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: kubernetes_pod_name

      - job_name: 'kube-state-metrics'
        static_configs:
          - targets: ['kube-state-metrics.kube-system.svc.cluster.local:8080']

      - job_name: 'kubernetes-cadvisor'

        scheme: https

        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        kubernetes_sd_configs:
        - role: node

        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor

      - job_name: 'kubernetes-service-endpoints'

        kubernetes_sd_configs:
        - role: endpoints

        relabel_configs:
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
          action: replace
          target_label: __scheme__
          regex: (https?)
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
          action: replace
          target_label: __address__
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
        - action: labelmap
          regex: __meta_kubernetes_service_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_service_name]
          action: replace
          target_label: kubernetes_name

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus-deployment
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus-server
  template:
    metadata:
      labels:
        app: prometheus-server
    spec:
      containers:
        - name: prometheus
          image: prom/prometheus:latest
          args:
            - "--config.file=/etc/prometheus/prometheus.yml"
            - "--storage.tsdb.path=/prometheus/"
          ports:
            - containerPort: 9090
          volumeMounts:
            - name: prometheus-config-volume
              mountPath: /etc/prometheus/
            - name: prometheus-storage-volume
              mountPath: /prometheus/
      volumes:
        - name: prometheus-config-volume
          configMap:
            defaultMode: 420
            name: prometheus-server-conf

        - name: prometheus-storage-volume
          emptyDir: {}

---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: monitoring
  labels:
    k8s-app: node-exporter
spec:
  selector:
    matchLabels:
      k8s-app: node-exporter
  template:
    metadata:
      labels:
        k8s-app: node-exporter
    spec:
      containers:
      - image: prom/node-exporter
        name: node-exporter
        ports:
        - containerPort: 9100
          protocol: TCP
          name: http
---
apiVersion: v1
kind: Service
metadata:
  labels:
    k8s-app: node-exporter
  name: node-exporter
  namespace: kube-system
spec:
  ports:
  - name: http
    port: 9100
    nodePort: 31672
    protocol: TCP
  type: NodePort
  selector:
    k8s-app: node-exporter

---
apiVersion: v1
kind: Service
metadata:
  name: prometheus-service
  namespace: monitoring
  annotations:
      prometheus.io/scrape: 'true'
      prometheus.io/port:   '9090'
spec:
  selector:
    app: prometheus-server
  type: NodePort
  ports:
    - port: 8080
      targetPort: 9090
      nodePort: 30003
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kube-state-metrics
subjects:
- kind: ServiceAccount
  name: kube-state-metrics
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: kube-state-metrics
rules:
- apiGroups:
  - ""
  resources: ["configmaps", "secrets", "nodes", "pods", "services", "resourcequotas", "replicationcontrollers", "limitranges", "persistentvolumeclaims", "persistentvolumes", "namespaces", "endpoints"]
  verbs: ["list","watch"]
- apiGroups:
  - extensions
  resources: ["daemonsets", "deployments", "replicasets", "ingresses"]
  verbs: ["list", "watch"]
- apiGroups:
  - apps
  resources: ["statefulsets", "daemonsets", "deployments", "replicasets"]
  verbs: ["list", "watch"]
- apiGroups:
  - batch
  resources: ["cronjobs", "jobs"]
  verbs: ["list", "watch"]
- apiGroups:
  - autoscaling
  resources: ["horizontalpodautoscalers"]
  verbs: ["list", "watch"]
- apiGroups:
  - authentication.k8s.io
  resources: ["tokenreviews"]
  verbs: ["create"]
- apiGroups:
  - authorization.k8s.io
  resources: ["subjectaccessreviews"]
  verbs: ["create"]
- apiGroups:
  - policy
  resources: ["poddisruptionbudgets"]
  verbs: ["list", "watch"]
- apiGroups:
  - certificates.k8s.io
  resources: ["certificatesigningrequests"]
  verbs: ["list", "watch"]
- apiGroups:
  - storage.k8s.io
  resources: ["storageclasses", "volumeattachments"]
  verbs: ["list", "watch"]
- apiGroups:
  - admissionregistration.k8s.io
  resources: ["mutatingwebhookconfigurations", "validatingwebhookconfigurations"]
  verbs: ["list", "watch"]
- apiGroups:
  - networking.k8s.io
  resources: ["networkpolicies"]
  verbs: ["list", "watch"]

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kube-state-metrics
  namespace: kube-system
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: kube-state-metrics
  name: kube-state-metrics
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kube-state-metrics
  template:
    metadata:
      labels:
        app: kube-state-metrics
    spec:
      containers:
      - image: quay.io/coreos/kube-state-metrics:v1.8.0
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 5
        name: kube-state-metrics
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 8081
          name: telemetry
        readinessProbe:
          httpGet:
            path: /
            port: 8081
          initialDelaySeconds: 5
          timeoutSeconds: 5
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: kube-state-metrics
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: kube-state-metrics
  name: kube-state-metrics
  namespace: kube-system
spec:
  clusterIP: None
  ports:
  - name: http-metrics
    port: 8080
    targetPort: http-metrics
  - name: telemetry
    port: 8081
    targetPort: telemetry
  selector:
    app: kube-state-metrics

```



### Grafana 연동

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      name: grafana
      labels:
        app: grafana
    spec:
      containers:
      - name: grafana
        image: grafana/grafana:latest
        ports:
        - name: grafana
          containerPort: 3000
        env:
        - name: GF_SERVER_HTTP_PORT
          value: "3000"
        - name: GF_AUTH_BASIC_ENABLED
          value: "false"
        - name: GF_AUTH_ANONYMOUS_ENABLED
          value: "true"
        - name: GF_AUTH_ANONYMOUS_ORG_ROLE
          value: Admin
        - name: GF_SERVER_ROOT_URL
          value: /
---
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: monitoring
  annotations:
      prometheus.io/scrape: 'true'
      prometheus.io/port:   '3000'
spec:
  selector:
    app: grafana
  type: NodePort
  ports:
    - port: 3000
      targetPort: 3000
      nodePort: 30004
```

```
kubectl apply -f grafana.yaml
```

```
kubectl get pod -n monitoring
```



- 그라파나 접속해서 data-sourece 정보를  kubectl get svc -n monitoring 으로 조회후 프로메테우스의 Cluster-ip로 적용

- dashborad import 315코드 사용

```
helm install grafana bitnami/grafana --values values.yaml
```

---

# HELM

https://github.com/bitnami/charts/tree/master/bitnami/kube-prometheus



```
helm repo add bitnami https://charts.bitnami.com/bitnami

helm install -f values.yaml prometheus bitnami/kube-prometheus
helm upgrade -f values.yaml prometheus bitnami/kube-prometheus
```



```
prometheus:
 persistence:
  enabled: true
 disableCompaction: true
 thanos:
  create: true
  extraArgs:
          #prometheusUrl: localhost:32483
  service:
   type: LoadBalancer
  objectStorageConfig:
   secretName: "thanos-objstore-config"
   secretKey: "thanos.yaml"
 service:
  type: NodePort
 scrapeInterval: 1s
 externalLabels:
  cluster: "cluster3"
```

```
global:
 storageClass: local-path
prometheus:
 persistence:
  enabled: true
 disableCompaction: true
 thanos:
  create: true
  extraArgs:
  service:
   type: LoadBalancer
 service:
  type: NodePort
 scrapeInterval: 1s
 externalLabels:
  cluster: "cluster1"
```



Target 

마스터노드

```
serviceMonitor/default/prometheus-kube-prometheus-alertmanager/0 (1/1 up)

serviceMonitor/default/prometheus-kube-prometheus-apiserver/0 (1/1 up)

serviceMonitor/default/prometheus-kube-prometheus-kube-controller-manager/0 (1/1 up)

serviceMonitor/default/prometheus-kube-prometheus-kube-scheduler/0 (1/1 up)


```



워커노드

```

```



# Prometheus Query (PromQL)



### Instant queries

```
GET /api/v1/query
POST /api/v1/query
```

- `query=<string>`: Prometheus expression query string.
- `time=<rfc3339 | unix_timestamp>`: Evaluation timestamp. Optional.
- `timeout=<duration>`: Evaluation timeout. Optional. Defaults to and is capped by the value of the `-query.timeout` flag.
- Grafana에서 기본 Query로 실행할때의 API 호출 예시. start, end가 아닌 특정 시간(time)이 query string으로 들어가 있다. resultType은 vector 이다.

```
192.168.150.115:30666/api/v1/query?query=node_cpu_seconds_total
```

- query=<string> 에 metric의 name을 넣어야함. 그라파나 or Prometheus UI에서 확인



---

### Range queries

```
GET /api/v1/query_range
POST /api/v1/query_range
```

- `query=<string>`: Prometheus expression query string.
- `start=<rfc3339 | unix_timestamp>`: Start timestamp.
- `end=<rfc3339 | unix_timestamp>`: End timestamp.
- `step=<duration | float>`: Query resolution step width in `duration` format or float number of seconds.
- `timeout=<duration>`: Evaluation timeout. Optional. Defaults to and is capped by the value of the `-query.timeout` flag.



```
192.168.150.115:30666/api/v1/query_range?start=1619657370&end=1619678970&step=15&query=node_cpu_seconds_total
```



### Finding series by label matchers

The following endpoint returns the list of time series that match a certain label set.

```
GET /api/v1/series
POST /api/v1/series
```

- `match[]=<series_selector>`: Repeated series selector argument that selects the series to return. At least one `match[]` argument must be provided.
- `start=<rfc3339 | unix_timestamp>`: Start timestamp.
- `end=<rfc3339 | unix_timestamp>`: End timestamp.

```
192.168.150.115:30666/api/v1/series?match[]=up
```



### Getting label names

```
GET /api/v1/labels
POST /api/v1/labels
```

- `start=<rfc3339 | unix_timestamp>`: Start timestamp. Optional.
- `end=<rfc3339 | unix_timestamp>`: End timestamp. Optional.
- `match[]=<series_selector>`: Repeated series selector argument that selects the series from which to read the label names. Optional.

```
192.168.150.115:30666/api/v1/labels
```



### Querying label values

```
GET /api/v1/label/<label_name>/values
```

- `start=<rfc3339 | unix_timestamp>`: Start timestamp. Optional.
- `end=<rfc3339 | unix_timestamp>`: End timestamp. Optional.
- `match[]=<series_selector>`: Repeated series selector argument that selects the series from which to read the label values. Optional.

```
192.168.150.115:30666/api/v1/label/job/values
```

- job은 label 로 조회한 내용중 하나

```
{
    "status": "success",
    "data": [
        "apiserver",
        "kubelet",
        "node-exporter",
        "prometheus-operator-alertmanager",
        "prometheus-operator-kube-proxy",
        "prometheus-operator-kube-state-metrics",
        "prometheus-operator-operator",
        "prometheus-operator-prometheus"
    ]
}

```



## Targets

The following endpoint returns an overview of the current state of the Prometheus target discovery:

```
GET /api/v1/targets
```

The `state` query parameter allows the caller to filter by active or dropped targets, (e.g., `state=active`, `state=dropped`, `state=any`). Note that an empty array is still returned for targets that are filtered out. Other values are ignored.

```
192.168.150.115:30666/api/v1/targets
```

```
192.168.150.115:30666/api/v1/targets?state=active
```



## Rules

The `/rules` API endpoint returns a list of alerting and recording rules that are currently loaded. In addition it returns the currently active alerts fired by the Prometheus instance of each alerting rule.

```
GET /api/v1/rules
```



## Alerts

The `/alerts` endpoint returns a list of all active alerts.

As the `/alerts` endpoint is fairly new, it does not have the same stability guarantees as the overarching API v1.

```
GET /api/v1/alerts
```



## Alertmanagers

```
GET /api/v1/alertmanagers
```



## Status

### Config

```
GET /api/v1/status/config
```



### Flags

```
GET /api/v1/status/flags
```



### Runtime Information

```
GET /api/v1/status/runtimeinfo
```



### Build Information

```
GET /api/v1/status/buildinfo
```



### TSDB Stats

```
GET /api/v1/status/tsdb
```



## TSDB Admin APIs

### Snapshot

Snapshot creates a snapshot of all current data into `snapshots/<datetime>-<rand>` under the TSDB's data directory and returns the directory as response. It will optionally skip snapshotting data that is only present in the head block, and which has not yet been compacted to disk.

```
POST /api/v1/admin/tsdb/snapshot
PUT /api/v1/admin/tsdb/snapshot
```

### Delete Series

DeleteSeries deletes data for a selection of series in a time range. The actual data still exists on disk and is cleaned up in future compactions or can be explicitly cleaned up by hitting the [Clean Tombstones](https://prometheus.io/docs/prometheus/latest/querying/api/#clean-tombstones) endpoint.

```
POST /api/v1/admin/tsdb/delete_series
PUT /api/v1/admin/tsdb/delete_series
```

### Clean Tombstones

CleanTombstones removes the deleted data from disk and cleans up the existing tombstones. This can be used after deleting series to free up space.

```
POST /api/v1/admin/tsdb/clean_tombstones
PUT /api/v1/admin/tsdb/clean_tombstones
```



---

```
100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[2s])) * 100)
```

```
kube_pod_container_info{cluster="cluster1"}
```

```
kube_pod_container_info{cluster="cluster1", namespace="default"}
```

```
100 - (avg by (pod, cluster, namespace) (rate(container_cpu_usage_seconds_total{namespace="fems-studio", cluster="pilot-poc-2"}[30s])) * 100)
```

```
sum (kube_namespace_labels{}) by (cluster, namespace)
```



클러스터 메모리

```
sum by(cluster)(container_memory_working_set_bytes{cluster="pilot-poc-2"})
```

```
sum(container_memory_usage_bytes{cluster="cluster1",namespace="",pod=""}) by (cluster,namespace,pod)
```



노드 메모리

```
(node_memory_MemTotal_bytes{cluster="cluster1"})
```

```
(node_memory_MemAvailable_bytes{cluster="cluster1"})
```

```
(node_memory_MemTotal_bytes{cluster="cluster1"})
```



디스크

```
node_filesystem_size_bytes{cluster="cluster1"}
```

```
node_filesystem_avail_bytes{cluster="cluster1"}
```

디스크 사용률

```
sum(node_filesystem_size_bytes{cluster="cluster1"} - node_filesystem_avail_bytes{cluster="cluster1"}) by (cluster) / sum(node_filesystem_size_bytes{cluster="cluster1"}) by (cluster) *100
```



클러스터당 네임스페이스 개수

```
count(kube_namespace_created) by (cluster)
```



클러스터당 파드 개수

```
count(count by (pod,cluster,namespace)(container_spec_memory_reservation_limit_bytes{pod!="",cluster="cluster1"}))
```



클러스터 별PVC 개수

```
count(kube_persistentvolumeclaim_info) by (cluster)
```



인스턴스별 CPU 점유율

```
sum by (instance) (irate(node_cpu_seconds_total{mode!~"guest.*|idle|iowait"}[5m])) *100
```



네트워크

Received

```
(sum by (cluster,namespace,pod) (rate (container_network_receive_bytes_total{cluster="cluster1"}[1m])))
```

Sent

```
- (sum by (cluster,namespace,pod) (rate (container_network_transmit_bytes_total{cluster="cluster1"}[1m])))
```



**Latency:** Latency can be extracted from the `apiserver_request_duration_seconds` histogram buckets:

```
sum by(verb,le) (rate(apiserver_request_duration_seconds_bucket[5m]))
```

```
histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{verb!~"WATCH|CONNECT"}[5m])) by (le) ) / 1e+06
```

```
 sum(rate(apiserver_request_duration_seconds_bucket[5m])) by (le) 
```

```
 sum(rate(apiserver_request_duration_seconds_bucket[5m])) / count(sum(rate(apiserver_request_duration_seconds_bucket[5m])) by (le))
```



**Request rate:** The metric ***apiserver_request_total\*** can be used to monitor the requests to the service, from where they are coming, to which service, which action and whether they were successful:

```
sum(rate(apiserver_request_total{job="kubernetes-apiservers",code=~"2.."}[5m]))
```

```
sum by (verb) (rate(apiserver_request_total[5m]))
```



1. API server latency
 
 ```
 sum(rate(apiserver_request_duration_seconds_bucket[5m])) / count(sum(rate(apiserver_request_duration_seconds_bucket[5m])) by (le))
 ```


2. API server total requests code

  ```
  sum by (code) (rate(apiserver_request_total[5m]))
  ```

3. API server total requests verb

  ```
  sum by (verb) (rate(apiserver_request_total[5m]))
  ```

4. API server errors

  ```
  sum by(verb) (rate(apiserver_request_total{code=~"5..",job="apiserver"}[5m])) / sum by(verb) (rate(apiserver_request_total{job="apiserver"}[5m]))
  ```

  

API request verb - get , list , create , update , patch , watch , delete , deletecollection 과 같은 리소스 요청에 사용하는 API 동사

5. API server requests by server

  ```
  sum(rate(apiserver_request_duration_seconds_count[5m])) by (instance)>0
  ```

  

Scheduler
1. Scheduler Attempts

  ```
  sum(increase(scheduler_schedule_attempts_total[1m])) by (result)
  ```

2. Scheduler Duration

   ```
   sum(rate(scheduler_e2e_scheduling_duration_seconds_sum[5m]))
   ```

3. Scheduling Failed Pods

  ```
  scheduler_pending_pods
  ```



# 기타

비트나미 helm 차트 사용 시 설정파일 접근을 위해서 exec 로 컨테이너안 조회하기

```
kubectl exec prometheus-prometheus-operator-prometheus-0 -c prometheus -- cat /etc/prometheus/prometheus.yml

prometheus-prometheus-operator-prometheus-0 : Pod명
```

 -c prometheus : 컨테이너 명



문제 발생

```
serviceMonitor/default/prometheus-kube-prometheus-kube-proxy/0 (0/3 up)
```

````
serviceMonitor/default/prometheus-kube-prometheus-kube-scheduler/0 (0/1 up)
```

```
serviceMonitor/default/prometheus-kube-prometheus-kube-controller-manager/0 (0/1 up)
```

검색결과 

Prometheus tried to got metrics from http://172.31.24.19:10251/metrics.
But scheduler port is exposed to only local 127.0.0.1 as follow:

즉, 메트릭 요청을 해당 아이피주소로 보내는데 Scheduler는 로컬호스트로만 expoese 되어 있다는 것. 



--> 문제해결 

https://kangwoo.github.io/devops/kubernetes/monitoring-kube-proxy-with-prometheus-operator/

`kube-proxy`도 `/metrics`라는 매트릭 엔드 포인트를 제공한다. 하지만 기본 설정값이 `127.0.0.1:10249`이기 때문에, 외부에서 접근이 안된다.

그래서 `prometheus`에서 수집하려고 하면, 접근이 안되서 문제가 발생한다.

```
$ kubectl -n kube-system edit cm/kube-proxy 
## Change from
    metricsBindAddress: 127.0.0.1:10249
## Change to
    metricsBindAddress: 0.0.0.0:10249
```

물론 `0.0.0.0:10249`로 변경하고, 모든 곳에서 다 접근이 가능하기 때문에, 보안이 취약한 곳이라면 사용하지 않는것이 좋다.

설정을 변경하고, `kube-proxy`를 재시작하면 적용이 된다.

```
kubectl -n kube-system delete pod -l k8s-app=kube-proxy 
```

즉, 127.0.0.1 -> 0.0.0.0 으로 바꾸고 파드 죽이고 다시 실행하면 적용됨



엔드포인트 확인

```
kubectl get ep -n kube-system
```

열린 포트 확인

```
ss -lnpt |egrep "kube-controller|kube-scheduler"
```



프로메테우스에서 잡는 controller 메트릭 포트는 10252

프로메테우스에서 잡는 scheduler 메트릭 포트는 10251



다음 명령어로 포트확인시 포트가 10259,10257 로 달라서 수정해줌

```
ss -lnpt |egrep "kube-controller|kube-scheduler"
```

```
vi /etc/kubernetes/manifests/kube-scheduler.yaml
vi /etc/kubernetes/manifests/kube-controller-manager.yaml
```



```
before

...
spec:
 containers:
 - command:
  - kube-scheduler
  - ...
  - ...
  - --port=0 #port 변경
  
after

...
spec:
 containers:
 - command:
  - kube-scheduler
  - ...
  - ...
  - --port=10251 #port 변경
```

자동으로 kube-system의 scheduler 파드가 재실행 됨





---

GPU Monitoring 을 위한 exporter



DCGM 

**D**ata **C**enter **G**PU **M**anager 의 약어로 Nvidia에서 제공하는 **Tesla 시리즈**의 그래픽 카드를 장착한 클러스터 또는 데이터센터 환경에서 비교적 가벼운 코스트로 GPU 를 모니터링하게 해주는 역할을 합니다.



helm repo add prometheus-community https://prometheus-community.github.io/helm-charts

- 설치 환경
  - OS : Ubuntu Linux 18.04
  - kubernetes version : **1.15.11 (master)** / 1.15.12(worker) , 1.16.15 (worker)
  - helm version : 3.3.1
  - Nvidia-driver version : 450.51.06
  - CUDA version : 11.0
  - GPU : RTX 3090, RTX 2080 Ti , GTX 1080, GTX 1080 Ti



1. window에서? telegraf 설치 해서 gpu 모니터링 agent

2. GPU 프로메테우스/ 텔레그래프  리눅스 환경에서 테스트
3. 프로메테우스 Nvidia 계열만 가능한것으로 확인



```
lspci | grep -i VGA
```

```
lspci -v | less
```

```
shw -numeric -C display
```



기타 등 



---

ServiceMonitor?

https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/getting-started.md



