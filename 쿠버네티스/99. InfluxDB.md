# influxdata

influxdb,telegraf,chronograf , kapacitor 설치 (ubuntu)

```
wget https://dl.influxdata.com/telegraf/releases/telegraf_1.17.2-1_amd64.deb
sudo dpkg -i telegraf_1.17.2-1_amd64.deb

wget https://dl.influxdata.com/influxdb/releases/influxdb2_2.0.3_amd64.deb
sudo dpkg -i influxdb2_2.0.3_amd64.deb

wget https://dl.influxdata.com/chronograf/releases/chronograf_1.8.9.1_amd64.deb
sudo dpkg -i chronograf_1.8.9.1_amd64.deb

wget https://dl.influxdata.com/kapacitor/releases/kapacitor_1.5.8-1_amd64.deb
sudo dpkg -i kapacitor_1.5.8-1_amd64.deb
```

다운로드 참고 https://portal.influxdata.com/downloads/



influxdb 실행

```
influxd
```



influxdb 접속

```
http://192.168.150.179:8086/
```



1.  도커 모니터링 연결해보기

![image-20210202185554154](C:\Users\INNOGRID\AppData\Roaming\Typora\typora-user-images\image-20210202185554154.png)



![image-20210202185626113](C:\Users\INNOGRID\AppData\Roaming\Typora\typora-user-images\image-20210202185626113.png)



![image-20210202185646279](C:\Users\INNOGRID\AppData\Roaming\Typora\typora-user-images\image-20210202185646279.png)



![image-20210202185726404](C:\Users\INNOGRID\AppData\Roaming\Typora\typora-user-images\image-20210202185726404.png)



![image-20210202185750098](C:\Users\INNOGRID\AppData\Roaming\Typora\typora-user-images\image-20210202185750098.png)



도커 데몬으로 돌리기

```
sudo service docker stop

sudo dockerd -H unix:///var/run/docker.sock -H tcp://192.168.150.179 -H tcp://10.2.0.64
sudo dockerd -H unix:///var/run/docker.sock -H tcp://10.2.0.64
도커의 localhost (DOCKER_HOST)는 0.0.0.0이다.

dockerd -H tcp://0.0.0.0:2375

혹은 이렇게 열어줘도 된다.

$ export DOCKER_HOST="tcp://0.0.0.0:2375"
 
$ docker ps

```

https://conservative-vector.tistory.com/entry/Remote-API-%EC%99%B8%EB%B6%80-%EC%BB%B4%ED%93%A8%ED%84%B0%EC%97%90%EC%84%9C%EB%8F%84-%EB%8F%84%EC%BB%A4%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%98%EA%B8%B0

중요!

```
도커데몬으로 전환하면서 기존 도커 서비스를 stop한 상태라서 kubernetes 연결 상태가 망가지는 듯????? -> 데몬으로 실행해놓은 상태에서 재연결 방법??? 
Flanel 로 변경해보고 
```



![image-20210203100209625](C:\Users\INNOGRID\AppData\Roaming\Typora\typora-user-images\image-20210203100209625.png)





위의 endpoint 입력란에 입력

```
endpoint = "unix:///var/run/docker.sock"

endpoint = "tcp://0.0.0.0:2375"

endpoint = "tcp://10.2.0.63:2375"
```



터미널에 화면에 나온 토큰과 config 를 복사해서 실행

![image-20210203100331753](C:\Users\INNOGRID\AppData\Roaming\Typora\typora-user-images\image-20210203100331753.png)





![image-20210202190022441](C:\Users\INNOGRID\AppData\Roaming\Typora\typora-user-images\image-20210202190022441.png)



![image-20210202190050695](C:\Users\INNOGRID\AppData\Roaming\Typora\typora-user-images\image-20210202190050695.png)



![image-20210202190121686](C:\Users\INNOGRID\AppData\Roaming\Typora\typora-user-images\image-20210202190121686.png)





2. 쿠버네티스 연결해보기



![image-20210203164113214](C:\Users\INNOGRID\AppData\Roaming\Typora\typora-user-images\image-20210203164113214.png)



![image-20210203164143583](C:\Users\INNOGRID\AppData\Roaming\Typora\typora-user-images\image-20210203164143583.png)



![image-20210203164418358](C:\Users\INNOGRID\AppData\Roaming\Typora\typora-user-images\image-20210203164418358.png)



url에 kubeproxy 주소 입력 -> 입력 후 나오는 토큰,agent 실행 문구 입력



E! [telegraf] Error running agent: could not initialize input inputs.kubernetes: open /run/secrets/kubernetes.io/serviceaccount/token: no such file or directory

시 해결 방법

(해결 중) -> 완료

kubectl get serviceaccount

kubectl describe serviceaccount default



/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory

https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#use-the-default-service-account-to-access-the-api-server



참고 사이트

https://community.influxdata.com/t/telegraf-kube-inventory-only-gathering-the-kubernetes-node-measurement/10237

https://portal.influxdata.com/downloads/



- chronograf 접속

```
http://192.168.150.179:8888/
```





----

# Helm Chart로 InfluxDB , Telegraf 사용



```
helm repo add influxdata https://helm.influxdata.com/
```

```
helm search repo influxdata
```

```
NAME                            CHART VERSION   APP VERSION     DESCRIPTION
influxdata/chronograf           1.1.24          1.8.9.1         Open-source web application written in Go and R...
influxdata/influxdb             4.9.14          1.8.4           Scalable datastore for metrics, events, and rea...
influxdata/influxdb-enterprise  0.1.12          1.8.0           Run InfluxDB Enterprise on Kubernetes
influxdata/influxdb2            1.1.0           2.0.4           A Helm chart for InfluxDB v2
influxdata/kapacitor            1.3.1           1.5.4           InfluxDB's native data processing engine. It ca...
influxdata/telegraf             1.7.37          1.17            Telegraf is an agent written in Go for collecti...
influxdata/telegraf-ds          1.0.20          1.17            Telegraf is an agent written in Go for collecti...
influxdata/telegraf-operator    1.1.5           v1.1.1          A Helm chart for Kubernetes to deploy telegraf-...
```

- influxdata/telegraf-ds <- demonset 각 노드에 1개씩 생성됨



infludxdb2 를 설치해야 UI가 제공된다.

```
helm show values influxdata/influxdb2
```

persistence 문제가 있다면 일단 비활성화 하자

```
helm install influxdb --set persistence.enabled=false influxdata/influxdb2
```

- 서비스 노드포트로 변경해서 접속



Retention Policy 를 설정해서 설정된 시간이 지난 데이터들은 자동으로 삭제시킨다.



추가 인자 확인

```
helm show values influxdata/telegraf-ds
```



values.yaml

```
override_config:
   toml: |+
     [global_tags]
       foo = "bar"
     [agent]
       interval = "10s"
     [[inputs.mem]]
     [[outputs.influxdb_v2]]
       urls           = ["http://192.168.150.115:32621"]
       bucket         = "default"
       organization   = "influxdata"
       token          = "hUEJW4ACdMD3U3SmvdXZUNbKMEZ53Y04"
```

- [[inputs.mem]] [[inputs.cpu]] 등 추가하면 알아서 수집함



```
helm install telegraf-ds --values values.yaml influxdata/telegraf-ds
```



프로메테우스를 수집해보자

기존의 influxdata/telegraf-ds 의 values.yaml 에서 input을 추가해주자.

```
override_config:
   toml: |+
     [global_tags]
       foo = "bar"
     [agent]
       interval = "10s"
     # Read metrics from one or many prometheus clients
     [[inputs.prometheus]]
       urls = ["http://10.2.0.11:9100/metrics","http://10.2.0.171:9100/metrics"]
       metric_version = 2
     [[outputs.influxdb_v2]]
       urls = ["http://192.168.150.116:31403"]
       token = "if17lNO05W6V33NBW40o7WTjuhfBHT1D"
       organization = "influxdata"
       bucket = "default"
```

- [[inputs.prometheus]] 밑 urls = [] 에 배열형식으로 수집할  metric 주소 설정 
- 잘 모르겠으면 prometheus UI의 Target 들어가서 확인하자 Helm으로 설치시 이미 자동으로 다 잡혀 있다. 



인플럭스DB UI 접속해서 확인

Data Explorer 에서 확인 Prometheus 및 추가한 해당 메트릭이 잡히는지 확인



InfluxDB 가능한 Input,Output 확인

https://docs.influxdata.com/telegraf/v1.18/plugins/#output-plugins

---

# Window 에서 수집해보자

설치 https://portal.influxdata.com/downloads/

### InfluxDB 2.0 오픈 소스 시계열 데이터베이스

```
wget https://dl.influxdata.com/influxdb/releases/influxdb2-2.0.6-windows-amd64.zip -UseBasicParsing -OutFile influxdb2-2.0.6-windows-amd64.zip
Expand-Archive .\influxdb2-2.0.6-windows-amd64.zip -DestinationPath 'C:\Program Files\InfluxData\influxdb2\'
```



설치 위치

```
C:\Program Files\InfluxData\influxdb2\influxdb2-2.0.6-windows-amd64
```

- cmd에서 influxd 실행
- 크롬에서 localhost:8086 접속
- 유저 및 organization, bucket 생성
- 로그인 후 token 및 output infludxdb 확인

```
> cd -Path C:\Program Files\InfluxData\influxdb
> ./influxd
```



### Telegraf 오픈 소스 데이터 수집기

```
wget https://dl.influxdata.com/telegraf/releases/telegraf-1.18.3_windows_amd64.zip -UseBasicParsing -OutFile telegraf-1.18.3_windows_amd64.zip
Expand-Archive .\telegraf-1.18.3_windows_amd64.zip -DestinationPath 'C:\Program Files\InfluxData\telegraf\'
```



설치 위치

```
C:\Program Files\InfluxData\telegraf\telegraf-1.18.3
```

- 폴더에서 telegraf.exe 한번 실행

- 편집기 아무거로 telegraf.conf 수정

- 시스템변수에 TELEGRAF_CONFIG_PATH 추가 경로는 telegraf.conf 로 지정

- cmd 창에서 telegraf 실행

- 만약 시스템변수 설정하고도 telegraf 실행 시 telegraf.conf 를 못찾는다는 오류 발생 시 C:\Program Files\Telegraf\telegraf.conf 경로대로 폴더 및 파일 생성 

  ( conf 파일 읽는 디폴트 위치가 C:\Program Files\Telegraf\telegraf.conf  인거 같음)



- cmd에서 telegraf 입력 및 실행



실행 확인 후 input 및 output 설정

기본적으로 telegraf.conf 파일 수정



input 및  output은 https://docs.influxdata.com/telegraf/v1.18/plugins/#input-plugins 에서 확인

아래 예시는 window 에서 nvidia 그래픽카드 gpu 정보를 수집하기 위한 설정



**주의! 공식 문서에서 다음과 같이 안내함 **

bin_path = "C:\\Windows\\System32\\nvidia-smi.exe" 에 아래와 같이 이스케이프 문자를 하나 더 넣어서 적용한다.

```
# Pulls statistics from nvidia GPUs attached to the host
[[inputs.nvidia_smi]]
  ## Optional: path to nvidia-smi binary, defaults to $PATH via exec.LookPath
  bin_path = "C:\\Windows\\System32\\nvidia-smi.exe"

  ## Optional: timeout for GPU polling
  # timeout = "5s"
```



```
# Telegraf Configuration
#
# Telegraf is entirely plugin driven. All metrics are gathered from the
# declared inputs, and sent to the declared outputs.
#
# Plugins must be declared in here to be active.
# To deactivate a plugin, comment out the name and any variables.
#
# Use 'telegraf -config telegraf.conf -test' to see what metrics a config
# file would generate.
#
# Environment variables can be used anywhere in this config file, simply surround
# them with ${}. For strings the variable must be within quotes (ie, "${STR_VAR}"),
# for numbers and booleans they should be plain (ie, ${INT_VAR}, ${BOOL_VAR})


# Global tags can be specified here in key="value" format.
[global_tags]
  # dc = "us-east-1" # will tag all metrics with dc=us-east-1
  # rack = "1a"
  ## Environment variables can be used as tags, and throughout the config file
  # user = "$USER"


# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "10s"
  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  ## Telegraf will send metrics to outputs in batches of at most
  ## metric_batch_size metrics.
  ## This controls the size of writes that Telegraf sends to output plugins.
  metric_batch_size = 1000

  ## Maximum number of unwritten metrics per output.  Increasing this value
  ## allows for longer periods of output downtime without dropping metrics at the
  ## cost of higher maximum memory usage.
  metric_buffer_limit = 10000

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
  collection_jitter = "0s"

  ## Default flushing interval for all outputs. Maximum flush_interval will be
  ## flush_interval + flush_jitter
  flush_interval = "10s"
  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "0s"

  ## By default or when set to "0s", precision will be set to the same
  ## timestamp order as the collection interval, with the maximum being 1s.
  ##   ie, when interval = "10s", precision will be "1s"
  ##       when interval = "250ms", precision will be "1ms"
  ## Precision will NOT be used for service inputs. It is up to each individual
  ## service input to set the timestamp at the appropriate precision.
  ## Valid time units are "ns", "us" (or "µs"), "ms", "s".
  precision = ""

  ## Log at debug level.
  # debug = false
  ## Log only error level messages.
  # quiet = false

  ## Log target controls the destination for logs and can be one of "file",
  ## "stderr" or, on Windows, "eventlog".  When set to "file", the output file
  ## is determined by the "logfile" setting.
  # logtarget = "file"

  ## Name of the file to be logged to when using the "file" logtarget.  If set to
  ## the empty string then logs are written to stderr.
  # logfile = ""

  ## The logfile will be rotated after the time interval specified.  When set
  ## to 0 no time based rotation is performed.  Logs are rotated only when
  ## written to, if there is no log activity rotation may be delayed.
  # logfile_rotation_interval = "0d"

  ## The logfile will be rotated when it becomes larger than the specified
  ## size.  When set to 0 no size based rotation is performed.
  # logfile_rotation_max_size = "0MB"

  ## Maximum number of rotated archives to keep, any older logs are deleted.
  ## If set to -1, no archives are removed.
  # logfile_rotation_max_archives = 5

  ## Override default hostname, if empty use os.Hostname()
  hostname = ""
  ## If set to true, do no set the "host" tag in the telegraf agent.
  omit_hostname = false


###############################################################################
#                            OUTPUT PLUGINS                                   #
###############################################################################


# Configuration for sending metrics to InfluxDB
#[[outputs.influxdb]]
  ## The full HTTP or UDP URL for your InfluxDB instance.
  ##
  ## Multiple URLs can be specified for a single cluster, only ONE of the
  ## urls will be written to each interval.
  # urls = ["unix:///var/run/influxdb.sock"]
  # urls = ["udp://127.0.0.1:8089"]
  # urls = ["http://127.0.0.1:8086"]

  ## The target database for metrics; will be created as needed.
  ## For UDP url endpoint database needs to be configured on server side.
  # database = "telegraf"

  ## The value of this tag will be used to determine the database.  If this
  ## tag is not set the 'database' option is used as the default.
  # database_tag = ""

  ## If true, the 'database_tag' will not be included in the written metric.
  # exclude_database_tag = false

  ## If true, no CREATE DATABASE queries will be sent.  Set to true when using
  ## Telegraf with a user without permissions to create databases or when the
  ## database already exists.
  # skip_database_creation = false

  ## Name of existing retention policy to write to.  Empty string writes to
  ## the default retention policy.  Only takes effect when using HTTP.
  # retention_policy = ""

  ## The value of this tag will be used to determine the retention policy.  If this
  ## tag is not set the 'retention_policy' option is used as the default.
  # retention_policy_tag = ""

  ## If true, the 'retention_policy_tag' will not be included in the written metric.
  # exclude_retention_policy_tag = false

  ## Write consistency (clusters only), can be: "any", "one", "quorum", "all".
  ## Only takes effect when using HTTP.
  # write_consistency = "any"

  ## Timeout for HTTP messages.
  # timeout = "5s"

  ## HTTP Basic Auth
  # username = "telegraf"
  # password = "metricsmetricsmetricsmetrics"

  ## HTTP User-Agent
  # user_agent = "telegraf"

  ## UDP payload size is the maximum packet size to send.
  # udp_payload = "512B"

  ## Optional TLS Config for use on HTTP connections.
  # tls_ca = "/etc/telegraf/ca.pem"
  # tls_cert = "/etc/telegraf/cert.pem"
  # tls_key = "/etc/telegraf/key.pem"
  ## Use TLS but skip chain & host verification
  # insecure_skip_verify = false

  ## HTTP Proxy override, if unset values the standard proxy environment
  ## variables are consulted to determine which proxy, if any, should be used.
  # http_proxy = "http://corporate.proxy:3128"

  ## Additional HTTP headers
  # http_headers = {"X-Special-Header" = "Special-Value"}

  ## HTTP Content-Encoding for write request body, can be set to "gzip" to
  ## compress body or "identity" to apply no encoding.
  # content_encoding = "identity"

  ## When true, Telegraf will output unsigned integers as unsigned values,
  ## i.e.: "42u".  You will need a version of InfluxDB supporting unsigned
  ## integer values.  Enabling this option will result in field type errors if
  ## existing data has been written.
  # influx_uint_support = false

# # Configuration for sending metrics to InfluxDB
 [[outputs.influxdb_v2]]
  ## The URLs of the InfluxDB cluster nodes.
  ##
  ## Multiple URLs can be specified for a single cluster, only ONE of the
  ## urls will be written to each interval.
  ## urls exp: http://127.0.0.1:9999
  urls = ["http://localhost:8086"]

  ## Token for authentication.
  token = "gCXPhJF6sAL90DLzc9oUM2OOmmn4Nm0ZGsPI5YInaZuRjK0mEV4wuldRMLBifRflaoGEy7ou85o398iMSaiD2A=="

  ## Organization is the name of the organization you wish to write to; must exist.
  organization = "test_org"

  ## Destination bucket to write into.
  bucket = "test_bucket"
#
#   ## The value of this tag will be used to determine the bucket.  If this
#   ## tag is not set the 'bucket' option is used as the default.
#   # bucket_tag = ""
#
#   ## If true, the bucket tag will not be added to the metric.
#   # exclude_bucket_tag = false
#
#   ## Timeout for HTTP messages.
#   # timeout = "5s"
#
#   ## Additional HTTP headers
#   # http_headers = {"X-Special-Header" = "Special-Value"}
#
#   ## HTTP Proxy override, if unset values the standard proxy environment
#   ## variables are consulted to determine which proxy, if any, should be used.
#   # http_proxy = "http://corporate.proxy:3128"
#
#   ## HTTP User-Agent
#   # user_agent = "telegraf"
#
#   ## Content-Encoding for write request body, can be set to "gzip" to
#   ## compress body or "identity" to apply no encoding.
#   # content_encoding = "gzip"
#
#   ## Enable or disable uint support for writing uints influxdb 2.0.
#   # influx_uint_support = false
#
#   ## Optional TLS Config for use on HTTP connections.
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false


###############################################################################
#                            INPUT PLUGINS                                    #
###############################################################################


# Windows Performance Counters plugin.
# These are the recommended method of monitoring system metrics on windows,
# as the regular system plugins (inputs.cpu, inputs.mem, etc.) rely on WMI,
# which utilize more system resources.
#
# See more configuration examples at:
#   https://github.com/influxdata/telegraf/tree/master/plugins/inputs/win_perf_counters

[[inputs.win_perf_counters]]
  [[inputs.win_perf_counters.object]]
    # Processor usage, alternative to native, reports on a per core.
    ObjectName = "Processor"
    Instances = ["*"]
    Counters = [
      "% Idle Time",
      "% Interrupt Time",
      "% Privileged Time",
      "% User Time",
      "% Processor Time",
      "% DPC Time",
    ]
    Measurement = "win_cpu"
    # Set to true to include _Total instance when querying for all (*).
    IncludeTotal=true
  
  [[inputs.win_perf_counters.object]]
    # Disk times and queues
    ObjectName = "LogicalDisk"
    Instances = ["*"]
    Counters = [
      "% Idle Time",
      "% Disk Time",
      "% Disk Read Time",
      "% Disk Write Time",
      "% Free Space",
      "Current Disk Queue Length",
      "Free Megabytes",
    ]
    Measurement = "win_disk"
    # Set to true to include _Total instance when querying for all (*).
    #IncludeTotal=false

  [[inputs.win_perf_counters.object]]
    ObjectName = "PhysicalDisk"
    Instances = ["*"]
    Counters = [
      "Disk Read Bytes/sec",
      "Disk Write Bytes/sec",
      "Current Disk Queue Length",
      "Disk Reads/sec",
      "Disk Writes/sec",
      "% Disk Time",
      "% Disk Read Time",
      "% Disk Write Time",
    ]
    Measurement = "win_diskio"

  [[inputs.win_perf_counters.object]]
    ObjectName = "Network Interface"
    Instances = ["*"]
    Counters = [
      "Bytes Received/sec",
      "Bytes Sent/sec",
      "Packets Received/sec",
      "Packets Sent/sec",
      "Packets Received Discarded",
      "Packets Outbound Discarded",
      "Packets Received Errors",
      "Packets Outbound Errors",
    ]
    Measurement = "win_net"

  [[inputs.win_perf_counters.object]]
    ObjectName = "System"
    Counters = [
      "Context Switches/sec",
      "System Calls/sec",
      "Processor Queue Length",
      "System Up Time",
    ]
    Instances = ["------"]
    Measurement = "win_system"
    # Set to true to include _Total instance when querying for all (*).
    #IncludeTotal=false

  [[inputs.win_perf_counters.object]]
    # Example query where the Instance portion must be removed to get data back,
    # such as from the Memory object.
    ObjectName = "Memory"
    Counters = [
      "Available Bytes",
      "Cache Faults/sec",
      "Demand Zero Faults/sec",
      "Page Faults/sec",
      "Pages/sec",
      "Transition Faults/sec",
      "Pool Nonpaged Bytes",
      "Pool Paged Bytes",
      "Standby Cache Reserve Bytes",
      "Standby Cache Normal Priority Bytes",
      "Standby Cache Core Bytes",
    ]
    # Use 6 x - to remove the Instance bit from the query.
    Instances = ["------"]
    Measurement = "win_mem"
    # Set to true to include _Total instance when querying for all (*).
    #IncludeTotal=false

  [[inputs.win_perf_counters.object]]
    # Example query where the Instance portion must be removed to get data back,
    # such as from the Paging File object.
    ObjectName = "Paging File"
    Counters = [
      "% Usage",
    ]
    Instances = ["_Total"]
    Measurement = "win_swap"

# Pulls statistics from nvidia GPUs attached to the host
[[inputs.nvidia_smi]]
  ## Optional: path to nvidia-smi binary, defaults to $PATH via exec.LookPath
  bin_path = "C:\\Windows\\System32\\nvidia-smi.exe"

  ## Optional: timeout for GPU polling
  # timeout = "5s"
  
# Windows system plugins using WMI (disabled by default, using
# win_perf_counters over WMI is recommended)


# # Read metrics about cpu usage
# [[inputs.cpu]]
#   ## Whether to report per-cpu stats or not
#   percpu = true
#   ## Whether to report total system cpu stats or not
#   totalcpu = true
#   ## If true, collect raw CPU time metrics.
#   collect_cpu_time = false
#   ## If true, compute and report the sum of all non-idle CPU states.
#   report_active = false


# # Read metrics about disk usage by mount point
# [[inputs.disk]]
#   ## By default stats will be gathered for all mount points.
#   ## Set mount_points will restrict the stats to only the specified mount points.
#   # mount_points = ["/"]
#
#   ## Ignore mount points by filesystem type.
#   ignore_fs = ["tmpfs", "devtmpfs", "devfs", "overlay", "aufs", "squashfs"]


# # Read metrics about disk IO by device
# [[inputs.diskio]]
#   ## By default, telegraf will gather stats for all devices including
#   ## disk partitions.
#   ## Setting devices will restrict the stats to the specified devices.
#   # devices = ["sda", "sdb", "vd*"]
#   ## Uncomment the following line if you need disk serial numbers.
#   # skip_serial_number = false
#   #
#   ## On systems which support it, device metadata can be added in the form of
#   ## tags.
#   ## Currently only Linux is supported via udev properties. You can view
#   ## available properties for a device by running:
#   ## 'udevadm info -q property -n /dev/sda'
#   # device_tags = ["ID_FS_TYPE", "ID_FS_USAGE"]
#   #
#   ## Using the same metadata source as device_tags, you can also customize the
#   ## name of the device via templates.
#   ## The 'name_templates' parameter is a list of templates to try and apply to
#   ## the device. The template may contain variables in the form of '$PROPERTY' or
#   ## '${PROPERTY}'. The first template which does not contain any variables not
#   ## present for the device is used as the device name tag.
#   ## The typical use case is for LVM volumes, to get the VG/LV name instead of
#   ## the near-meaningless DM-0 name.
#   # name_templates = ["$ID_FS_LABEL","$DM_VG_NAME/$DM_LV_NAME"]


# # Read metrics about memory usage
# [[inputs.mem]]
#   # no configuration


# # Read metrics about swap memory usage
# [[inputs.swap]]
#   # no configuration

```



---

# 리눅스에서 수집해보자



```
# Pulls statistics from nvidia GPUs attached to the host
[[inputs.nvidia_smi]]
  ## Optional: path to nvidia-smi binary, defaults to $PATH via exec.LookPath
  # bin_path = "/usr/bin/nvidia-smi"

  ## Optional: timeout for GPU polling
  # timeout = "5s"
```







---

# 쿠버네티스에서 수집해보자







# SYSSTAT을 수집해보자

```
# Sysstat metrics collector
[[inputs.sysstat]]
  ## Path to the sadc command.
  #
  ## On Debian and Arch Linux the default path is /usr/lib/sa/sadc whereas
  ## on RHEL and CentOS the default path is /usr/lib64/sa/sadc
  sadc_path = "/usr/lib/sa/sadc" # required

  ## Path to the sadf command, if it is not in PATH
  # sadf_path = "/usr/bin/sadf"

  ## Activities is a list of activities, that are passed as argument to the
  ## sadc collector utility (e.g: DISK, SNMP etc...)
  ## The more activities that are added, the more data is collected.
  # activities = ["DISK"]

  ## Group metrics to measurements.
  ##
  ## If group is false each metric will be prefixed with a description
  ## and represents itself a measurement.
  ##
  ## If Group is true, corresponding metrics are grouped to a single measurement.
  # group = true

  ## Options for the sadf command. The values on the left represent the sadf options and
  ## the values on the right their description (wich are used for grouping and prefixing metrics).
  ##
  ## Run 'sar -h' or 'man sar' to find out the supported options for your sysstat version.
  [inputs.sysstat.options]
	-C = "cpu"
	-B = "paging"
	-b = "io"
	-d = "disk"             # requires DISK activity
	"-n ALL" = "network"
	"-P ALL" = "per_cpu"
	-q = "queue"
	-R = "mem"
	-r = "mem_util"
	-S = "swap_util"
	-u = "cpu_util"
	-v = "inode"
	-W = "swap"
	-w = "task"
  #	-H = "hugepages"        # only available for newer linux distributions
  #	"-I ALL" = "interrupts" # requires INT activity

  ## Device tags can be used to add additional tags for devices. For example the configuration below
  ## adds a tag vg with value rootvg for all metrics with sda devices.
  # [[inputs.sysstat.device_tags.sda]]
  #  vg = "rootvg"
```

sar를 방금 설치하고 구성한 경우 디렉토리를 가져 오지 못할 수 있으며, 하루 종일 보고서가 자정에 폴더에 기록됩니다.



 vi /etc/default/sysstat
ENABLED="true"
sudo service sysstat restart

/usr/lib/sysstat/sadc



```
override_config:
   toml: |+
     [global_tags]
       foo = "bar"
     [agent]
       interval = "100ms"
       metric_buffer_limit = 100000
       flush_interval = "5s"

     [[inputs.sysstat]]
      sadc_path = "/usr/lib/sysstat/sadc" # required
      sadf_path = "/usr/bin/sadf"
      activities = ["DISK", "SNMP", "INT"]
      [inputs.sysstat.options]
            -C = "cpu"
            -B = "paging"
            -b = "io"
            -d = "disk"             # requires DISK activity
            -H = "hugepages"
            "-I ALL" = "interrupts" # requires INT activity
            "-n ALL" = "network"
            "-P ALL" = "per_cpu"
            -q = "queue"
            -R = "mem"
            "-r ALL" = "mem_util"
            -S = "swap_util"
            -u = "cpu_util"
            -v = "inode"
            -W = "swap"
            -w = "task"

     [[inputs.cpu]]
     [[inputs.mem]]
     [[inputs.diskio]]
     [[inputs.swap]]
     [[inputs.netstat]]
     #[[inputs.prometheus]]
     #  urls = ["http://10.2.0.159:9100/metrics","http://10.2.0.172:9100/metrics","http://10.44.0.2:8080/metrics"]
     # metric_version = 2
     #kubernetes_services = ["http://prometheus-node-exporter.default:9100/metrics","http://prometheus-kube-prometheus-kube-scheduler.kube-system:10251/metrics","http://prometheus-kube-prometheus-kube-proxy.kube-system:10252/metrics"]
     [[outputs.influxdb_v2]]
       urls = ["http://192.168.150.115:30640"]
       token = "NTDNyye9GrClYL3cbCyA3btQs3vyRBci"
       organization = "influxdata"
       bucket = "default"

```



# CPU부하를 측정해보자

https://docs.influxdata.com/telegraf/v1.18/plugins/#input-plugins

```
helm upgrade telegraf -f telegraf.yaml influxdata/telegraf-ds
```

```
override_config:
   toml: |+
     [global_tags]
       foo = "bar"
     [agent]
       interval = "100ms"
       metric_buffer_limit = 100000
       flush_interval = "5s"

     [[inputs.cpu]]
     [[inputs.mem]]
     [[inputs.diskio]]
     [[inputs.swap]]
     [[inputs.netstat]]
     #[[inputs.prometheus]]
     #  urls = ["http://10.2.0.159:9100/metrics","http://10.2.0.172:9100/metrics","http://10.44.0.2:8080/metrics"]
     # metric_version = 2
     #kubernetes_services = ["http://prometheus-node-exporter.default:9100/metrics","http://prometheus-kube-prometheus-kube-scheduler.kube-system:10251/metrics","http://prometheus-kube-prometheus-kube-proxy.kube-system:10252/metrics"]
     [[outputs.influxdb_v2]]
       urls = ["http://192.168.150.115:30640"]
       token = "NTDNyye9GrClYL3cbCyA3btQs3vyRBci"
       organization = "influxdata"
       bucket = "default"

```



stress 설치

```
apt-get install stress
```

#### CPU

Stress -c <코어수> - 지정한 코어를 100% 사용하도록 합니다.

ex) stress -c 4

 

\- 작업창을 한 개 더 열어서 top 명령어를 실행하고 숫자 1을 누르시면 코어 수를 확인할 수 있습니다. stress 프로그램으로 코어의 수를 지정하면, 정확하게 지정한 코어 수는 % CPU 목록에 100% 사용을 확인할 수 있습니다.

 

#### Memory

stress --vm <프로세스 개수> --vm-bytes <사용할 크기> - 메모리 로드를 위해 프로세스의 수와, 사용할 메모리의 크기를 설정합니다.

ex) stress --vm 2 --vm-bytes 2048m

 

\- memory load를 위해 2개의 process와 2048MB의 메모리를 사용합니다. 정상적으로 진행되지 않는다면, 로드하려는 메모리의 크기를 너무 과하게 설정된 경우일 수 있습니다.

 

#### HDD

stress --hdd --hdd-bytes <사용할 크기> - 하드 디스크 로드를 위한 프로세스의 수와 테스트 파일의 크기를 지정합니다.

ex) stress --hdd 2 --hdd-bytes 1024m

 

-hard disk load를 위해서 2개의 process와 1024MB 용량의 파일을 사용합니다.

 

#### Test all

ex) stress -c 4 --vm 3 --vm-bytes 2048m --hdd 2 --hdd-bytes 1024m --timeout 60s

 

#### ETC Option

-t N, --timeout N - N초간 테스트 진행, 시간을 지정하지 않을 시에는 취소할 때까지 계속 진행됩니다.

Info stress - stress 명령어의 더 자세한 사용법을 볼 수 있습니다.

 

간단하게 ubuntu에서 시스템에 부하를 줄 수 있는 stress test에 대해서 알아보았습니다.

그러면 이번에는 CPU, MEM, I/O, Network 등의 디바이스가 얼마나 사용되고, 활동하고 있는지 알아보는 명령어를 설명해드리겠습니다.



출처: https://jcil.co.kr/13 [또이리의 Server Engineer]



---

# InfludxDB에 외부에서 API로 값을 받아보자

https://docs.influxdata.com/influxdb/v2.0/api/#operation/GetOrgs



postman 에서 사용시

## [Authentication](https://docs.influxdata.com/influxdb/v2.0/reference/api/#authentication)

InfluxDB uses [authentication tokens](https://docs.influxdata.com/influxdb/v2.0/security/tokens/) to authorize API requests. Include your authentication token as an `Authorization` header in each request.

```sh
curl --request POST http://localhost:8086/api/v2/write \
  --header "Authorization: Token YOURAUTHTOKEN" \
  --data-urlencode "org=myorg" \
  --data-urlencode "bucket=example-bucket"
```



Autentication을 추가하지 않으면 unauthorization 오류 발생

1. Header에 KEY: Authorization Value: Token NTDNyye9GrClYL3cbCyA3btQs3vyRBci 입력



InfludxDB에서 Build Query는 flux 형태니까 flux type으로 요청하는게 편할지도?



```bash
curl --request POST \
  http://192.168.150.115:30640/api/v2/query?org=influxdata  \
  --header 'Authorization: Token NTDNyye9GrClYL3cbCyA3btQs3vyRBci' \
  --header 'Accept: application/csv' \
  --header 'Content-type: application/vnd.flux' \
  --data 'from(bucket: "default")
        |> range(start: v.timeRangeStart, stop: v.timeRangeStop)
        |> filter(fn: (r) => r["_measurement"] == "cpu")
        |> filter(fn: (r) => r["_field"] == "usage_idle")
        |> filter(fn: (r) => r["cpu"] == "cpu-total")
        |> aggregateWindow(every: v.windowPeriod, fn: mean, createEmpty: false)
        |> yield(name: "mean")'
```

```
curl --request POST \
  http://192.168.150.115:30640/api/v2/query?org=influxdata  \
  --header 'Authorization: Token NTDNyye9GrClYL3cbCyA3btQs3vyRBci' \
  --header 'Accept: application/csv' \
  --header 'Content-type: application/vnd.flux' \
  --data 'from(bucket:"default")
        |> range(start: -12h)
        |> filter(fn: (r) => r["_measurement"] == "cpu")
        |> filter(fn: (r) => r["_field"] == "usage_idle")
        |> filter(fn: (r) => r["cpu"] == "cpu-total")
        |> aggregateWindow(every: 10m, fn: mean, createEmpty: false)
        |> yield(name: "mean")'
```



infludxDB 2.0 Api docs

https://docs.influxdata.com/influxdb/v2.0/api/#operation/PostQuery



postman

```
http://192.168.150.115:30640/api/v2/query?org=influxdata
```



Params

```
Key: org
Value: influxdata
```



Headers

```
Content-Type : application/vnd.flux
Authorization : Token NTDNyye9GrClYL3cbCyA3btQs3vyRBci
Zap-Trace-Span : trace_id
Accept : application/json
```



Body - raw (JSON)

```
from(bucket:"default")
        |> range(start: -12h)
        |> filter(fn: (r) => r["_measurement"] == "cpu")
        |> filter(fn: (r) => r["_field"] == "usage_idle")
        |> filter(fn: (r) => r["cpu"] == "cpu-total")
        |> aggregateWindow(every: 10m, fn: mean, createEmpty: false)
        |> yield(name: "mean")
```

